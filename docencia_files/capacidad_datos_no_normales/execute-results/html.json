{
  "hash": "14292d20b3077669fc86abe8c646f0cb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Análisis de capacidad para datos no normales\"\ndate: today\nformat:\n  html:\n    toc: true\n    collapse-sections: true\nexecute:\n  warning: false\n  message: false\n---\n\n# Pasos previos\n\nEl paquete `qualityTools` ya no está disponible para instalación directa desde RStudio, sino que debe instalarse de la siguiente forma. Lo mismo sucede para el paquete `Johnson`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\n\ndevtools::install_github(\"cran/qualityTools\")\ndevtools::install_github(\"hrbrmstr/Johnson\")\n```\n:::\n\n\n# Introducción\n\nEs relativamente común encontrarse con procesos que no siguen la distribución normal, ya sea por que así es como debe comportarse o bien, por errores que suceden a la hora de recolectar, almacenar o procesar los datos. \n\nLo aquí abordado corresponde al proceder cuando la distribución es intrinsicamente no normal. A grandes razgos se van a tratar los siguientes tópicos.\n\n* Transformaciones no lineales\n* Método de los percentiles\n\n# Librerías\n\nLas librerías que vamos a usar en el desarrollo de estos ejemplos se enlistan a continuación:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(qualityTools)\nlibrary(Johnson) # transformación de Johnson\nlibrary(MASS) # transformación Box-Cox\nlibrary(car) # algunos gráficos\nlibrary(qcc) \n```\n:::\n\n\n# Análisis de capacidad para datos no normales\n\nPara el desarrollo de este ejemplo se van a generar, de forma intencional, datos que no siguen la distribución normal. \n\nEn este caso será una distribución Weibull, con parámetros forma y escala.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20250512)\n\ndatos_no_normales <- rweibull(100, shape = 1.5, scale = 3)\n\nhist(datos_no_normales, \n     main = \"Histograma de datos no normales\", \n     col = \"darkred\", \n     xlab = \"Datos no normales\", \n     ylab = \"Frecuencia\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nComo se puede apreciar, los datos no siguen la distribución normal \n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(datos_no_normales)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  datos_no_normales\nW = 0.92514, p-value = 2.664e-05\n```\n\n\n:::\n\n```{.r .cell-code}\ncar::qqPlot(datos_no_normales, \n            main = \"Gráfico cuantil-cuantil\",\n            xlab = \"Cuántiles teóricos\",\n            ylab = \"Cuantiles\",\n            col.lines = \"darkred\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 92 94\n```\n\n\n:::\n:::\n\n\nSuponga que las especificaciones son $3 \\pm 2$. Y que el proceso se encuentra bajo control, aunque vayamos a generar algunos gráficos para comprobarlo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(datos_no_normales, \n     main = \"Histograma de datos no normales\", \n     col = \"darkred\", \n     xlab = \"Datos no normales\", \n     ylab = \"Frecuencia\")\n\nabline(v = c(1, 3, 5), col = \"gold3\", lwd = 3.5, lty = 2)\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## Transformaciones no lineales\n\nEl primer abordaje corresponde al uso de transformaciones no lineales.\n\n### Box-Cox\n\nRecuerde que esta familia de transformaciones sigue la siguiente fórmula.\n\n$$\nw_i=\\Biggl\\{ \\begin{matrix} x_i^\\lambda \\quad si \\quad \\lambda \\ne 0 \\\\ log(x_i) \\quad si \\quad \\lambda = 0\\end{matrix}, \\text{para } x_i > 0\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbc <- MASS::boxcox(lm(datos_no_normales ~ 1))\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlambda <- bc$x[which.max(bc$y)]\n\nlambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4242424\n```\n\n\n:::\n:::\n\n\nAlgunos software trabajan directamente con los valores redondeados, por lo que en este caso $\\lambda=-2$\n\nAhora, apliquemos la transformación\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sin redondeo\ndatos_transformados_1 <- datos_no_normales^lambda\n\nshapiro.test(datos_transformados_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  datos_transformados_1\nW = 0.99547, p-value = 0.9855\n```\n\n\n:::\n\n```{.r .cell-code}\n# Con redondeo\ndatos_transformados_2 <- datos_no_normales^0.5\nshapiro.test(datos_transformados_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  datos_transformados_2\nW = 0.99494, p-value = 0.9735\n```\n\n\n:::\n:::\n\n\nNótese como en ambas situaciones, los datos ahora si siguen la distribución normal. Ahora analicemos la capacidad.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXR <- matrix(datos_transformados_1, ncol = 4) %>% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Box - Cox\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nqcc::process.capability(XR, \n                        spec.limits = c(1, 5),\n                        target = 3, \n                        breaks = \"sturges\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(1, 5), target = 3,     breaks = \"sturges\")\n\nNumber of obs = 100          Target = 3\n       Center = 1.458           LSL = 1\n       StdDev = 0.44            USL = 5\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    1.5152  1.3043  1.7257\nCp_l  0.3467  0.2786  0.4149\nCp_u  2.6837  2.3652  3.0021\nCp_k  0.3467  0.2655  0.4280\nCpm   0.4157  0.3359  0.4953\n\nExp<LSL 15%\t Obs<LSL 14%\nExp>USL 0%\t Obs>USL 0%\n```\n\n\n:::\n:::\n\n\nPero hay algo raro, ¿no? Es porque aún no transformamos los límites y el nominal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ponga atención, a veces la transformación invierte los límites\n\nc(1, 3, 5)^lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.000000 1.593731 1.979396\n```\n\n\n:::\n\n```{.r .cell-code}\nqcc::process.capability(XR, \n                        spec.limits = c(1, 1.979396),\n                        target = 1.593731, \n                        breaks = \"sturges\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(1, 1.979396),     target = 1.593731, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 1.594\n       Center = 1.458           LSL = 1\n       StdDev = 0.44            USL = 1.979\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3710  0.3194  0.4225\nCp_l  0.3467  0.2786  0.4149\nCp_u  0.3952  0.3235  0.4669\nCp_k  0.3467  0.2655  0.4280\nCpm   0.3544  0.3033  0.4055\n\nExp<LSL 15%\t Obs<LSL 14%\nExp>USL 12%\t Obs>USL 13%\n```\n\n\n:::\n:::\n\n\n### Johnson\n\n>Nota: este paquete `Johnson` fue removido del CRAN (repositorio oficial), y en general suele ser algo deficiente en el cálculo de las transformaciones.\n\nEsta está compuesta por tres familias de transformaciones: $S_B, S_L, S_U$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatos_transformados_3 <- Johnson::RE.Johnson(datos_no_normales)\n\nshapiro.test(datos_transformados_3$transformed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  datos_transformados_3$transformed\nW = 0.99375, p-value = 0.929\n```\n\n\n:::\n:::\n\n\nHabiendo verificado que la transformación es efectiva, podemos proceder a realizar el análisis de capacidad.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# verificamos la familia de transformaciones\n\ndatos_transformados_3$`function`\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SB\"\n```\n\n\n:::\n:::\n\n\nSabiendo que es $S_B$, se puede proceder según la fórmula\n\n$$\nw_i = \\gamma+\\eta\\cdot ln\\Big[\\frac{x_i-\\varepsilon}{\\lambda+\\varepsilon-x_i}\\Big]\n$$\n\nEntonces:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(g <- datos_transformados_3$f.gamma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.081273\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(l <- datos_transformados_3$f.lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 24.67992\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eta <- datos_transformados_3$f.eta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.690125\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ep <- datos_transformados_3$f.epsilon)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9426102\n```\n\n\n:::\n\n```{.r .cell-code}\nx_i <- c(1, 3, 5)\n\ng + (eta * log((x_i-ep)/(l+ep-x_i)))  #log = ln\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.0763915  0.2755111  1.1403874\n```\n\n\n:::\n:::\n\n\nPor lo tanto: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nXR <- matrix(datos_transformados_3$transformed, ncol = 4) %>% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Johnson\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nqcc::process.capability(XR, \n                        spec.limits = c(-1.0763915, 1.1403874),\n                        target = 0.2755111, \n                        breaks = \"sturges\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(-1.0763915,     1.1403874), target = 0.2755111, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 0.2755\n       Center = -0.02526        LSL = -1.076\n       StdDev = 0.9774          USL = 1.14\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3780  0.3254  0.4305\nCp_l  0.3585  0.2895  0.4275\nCp_u  0.3975  0.3256  0.4694\nCp_k  0.3585  0.2762  0.4407\nCpm   0.3613  0.3091  0.4133\n\nExp<LSL 14%\t Obs<LSL 14%\nExp>USL 12%\t Obs>USL 13%\n```\n\n\n:::\n:::\n\n\n### Transformación en dos pasos\n\nEn Excel también puede encontrar una forma de abordar este caso, pues es relativamente sencillo de aplicar.\n\n<a href=\"datos/Transformación en dos pasos en Excel.xlsx\" download>Descargar Transformación en dos pasos con Excel</a>\n\nDada algunas pequeñas diferencias en los algoritmos programados, encontrará disimilitudes en algunos de los resultados obtenidos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndos_pasos <- bestNormalize::bestNormalize(datos_no_normales)\n\ndatos_transformados_4 <- dos_pasos$x.t * sd(datos_no_normales) +\n  mean(datos_no_normales)\n\nshapiro.test(datos_transformados_4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  datos_transformados_4\nW = 0.98477, p-value = 0.3055\n```\n\n\n:::\n:::\n\n\nComo puede observar, la transformación es efectiva. Ahora se deben obtener los límites. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlimites <- predict(dos_pasos, \n                   newdata = c(1, 3, 5), \n                   inverse = F) * sd(datos_no_normales) + \n  mean(datos_no_normales)\n\nlimites\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7706979 3.5210154 4.9708862\n```\n\n\n:::\n:::\n\n\nResuelto esto, calculemos la capacidad\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXR <- matrix(datos_transformados_4, ncol = 4) %>% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Dos pasos\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nqcc::process.capability(XR, \n                        spec.limits = c(0.7706979, 4.9708862),\n                        target = 3.5210154, \n                        breaks = \"sturges\")\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-16-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(0.7706979,     4.9708862), target = 3.5210154, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 3.521\n       Center = 2.813           LSL = 0.7707\n       StdDev = 1.835           USL = 4.971\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3814  0.3283  0.4344\nCp_l  0.3709  0.3010  0.4408\nCp_u  0.3919  0.3204  0.4633\nCp_k  0.3709  0.2877  0.4542\nCpm   0.3559  0.3035  0.4081\n\nExp<LSL 13%\t Obs<LSL 14%\nExp>USL 12%\t Obs>USL 13%\n```\n\n\n:::\n:::\n\n\n## Método de los percentiles\n\nPara aplicar este método primero debe identificarse la distribución subyacente que mejor se ajusta a los datos. Esto se puede hacer de muchas formas, esta es una de ellas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistrplus::descdist(datos_no_normales)\n```\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsummary statistics\n------\nmin:  0.05970522   max:  10.59864 \nmedian:  2.457949 \nmean:  2.813116 \nestimated sd:  1.911338 \nestimated skewness:  1.168548 \nestimated kurtosis:  4.996266 \n```\n\n\n:::\n:::\n\n\nDel gráfico se muestra que la Weibull, lognormal y gamma son posibles candidatos. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# probamos varias distribuciones\n\ndistribuciones <- c(\"weibull\", \"lnorm\", \"exp\", # agregar más si es conveniente\n  \"gamma\", \"unif\", \"logis\") %>% \n  purrr::map(function(x){\n    fitdistrplus::fitdist(datos_no_normales, x)\n    })\n\nbondad_ajuste <- fitdistrplus::gofstat(distribuciones)\n\nround(bondad_ajuste$chisqpvalue,3) # valores p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1-mle-weibull   2-mle-lnorm     3-mle-exp   4-mle-gamma    5-mle-unif \n        0.920         0.051         0.015         0.755         0.000 \n  6-mle-logis \n        0.798 \n```\n\n\n:::\n\n```{.r .cell-code}\nbondad_ajuste$kstest # decisiones kolmogorov\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 1-mle-weibull    2-mle-lnorm      3-mle-exp    4-mle-gamma     5-mle-unif \n\"not rejected\" \"not rejected\"     \"rejected\" \"not rejected\"     \"rejected\" \n   6-mle-logis \n\"not rejected\" \n```\n\n\n:::\n\n```{.r .cell-code}\nbondad_ajuste$adtest # decisiones anderson - darling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 1-mle-weibull    2-mle-lnorm      3-mle-exp    4-mle-gamma     5-mle-unif \n\"not rejected\" \"not computed\"     \"rejected\" \"not rejected\" \"not computed\" \n   6-mle-logis \n    \"rejected\" \n```\n\n\n:::\n:::\n\n\nObservando los resultados se intuye que se puede usar la Weibull o la gamma. Como ya sabemos que los datos provenienen de una Weibull (además de su alto valor P), se va a trabajar con ello.\n\n#### Estimación con `qualityTools`\n\n> Este paquete no está en CRAN, su instalación es manual. Suele presentar errores en versiones actuales de `R`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresultado <- qualityTools::cp(datos_no_normales,\n                 distribution = \"weibull\", \n                 lsl = 1,\n                 target = 3,\n                 usl = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAnderson Darling Test for weibull distribution\n\ndata:  datos_no_normales \n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in round(x$statistic, 4): non-numeric argument to mathematical function\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](capacidad_datos_no_normales_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n#### Estimación manual\n\nA grosso modo, recordemos que la fórmula es:\n\n$$\nC_p(q) = \\frac{USL - LSL}{x_{0.99865}-x_{0.00135}}\n$$\n\nCon sus respectivas variantes para $C_{pk}$.\n\nSabiendo que la distribución es Weibull, se van a estimar sus parámetros.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitdistrplus::fitdist(datos_no_normales, \"weibull\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFitting of the distribution ' weibull ' by maximum likelihood \nParameters:\n      estimate Std. Error\nshape 1.494667  0.1165100\nscale 3.110131  0.2188012\n```\n\n\n:::\n\n```{.r .cell-code}\npercentiles <- qweibull(c(0.99865, 0.00135, 0.5), # superior, inferior, mediana \n                        shape = 1.494667, scale = 3.110131)\n\n# Entonces Cp\n\n(5-1)/(percentiles[1]-percentiles[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3648482\n```\n\n\n:::\n\n```{.r .cell-code}\n# Y por tanto Cpk\n\nmin((5-percentiles[3])/(percentiles[1]-percentiles[3]), #cpu\n    (percentiles[3]-1)/(percentiles[3]-percentiles[2])) #cpl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2995426\n```\n\n\n:::\n:::\n\n\n\n# ¿Cómo modificar valores?\n\nSi se desea agregar otros parámetros en las funciones, puede consultar la documentación de la función con `help(qualityTools)` u otro paquete (cambiando el nombre)\n",
    "supporting": [
      "capacidad_datos_no_normales_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}