[
  {
    "objectID": "tutoriales_R/introduccionR.html",
    "href": "tutoriales_R/introduccionR.html",
    "title": "Introducción a R y RStudio",
    "section": "",
    "text": "Para los que nunca han programado, o al menos nunca han programado en R/Rstudio.\nEsta guía está basada en este libro."
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#instalación-de-r-y-rstudio",
    "href": "tutoriales_R/introduccionR.html#instalación-de-r-y-rstudio",
    "title": "Introducción a R y RStudio",
    "section": "Instalación de R y RStudio",
    "text": "Instalación de R y RStudio\n\n[Última versión de R] en función del sistema operativo\nÚltima versión de RStudio"
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#partes-de-la-interfaz-de-rstudio",
    "href": "tutoriales_R/introduccionR.html#partes-de-la-interfaz-de-rstudio",
    "title": "Introducción a R y RStudio",
    "section": "Partes de la interfaz de RStudio",
    "text": "Partes de la interfaz de RStudio\n\n\n\nInterfaz de RStudio\n\n\n\nDescripción de las partes de RStudio\n\n\n\n\n\n\n\nPanel de scripts: es donde va todo el código especializado que vamos creando y queremos conservar cuando terminamos la jornada de trabajo. Cuando se abre RStudio puede que este panel no esté visible, porque no hay scripts en él. Sobre todo la primera vez (nuevos usuarios).\n\n\nPanel de ambiente: acá se muestran todos los objetos, variables y funciones que se vayan creando. Como pueden notar, tiene otras pestañas, que no son de interés para el nivel de esta introducción.\n\n\n\n\nPanel de consola: básicamente, el código que se ejecute en consola (Console) no se va a guardar, ni puede ser editado. La idea es usar la consola para ejecutar código que solo se va a usar una vez, por ejemplo, instalar un paquete, correr una prueba, entre otros. También tiene otras pestañas que pueden variar según la versión de RStudio instalada, por ejemplo Terminal y Background Jobs, pero de igual forma no son de interés para el nivel de esta introducción.\n\n\nPanel “general”: en este se muestran muchas cosas:\n\nFiles: la carpeta en la que están trabajando, con todos los archivos que contenga.\nPlots: acá se van a mostrar los gráficos que se vayan ejecutando, se van guardando por lo que se pueden ver los antiguos, pero por lo general es más fácil volver a ejecutar el código, que navegar por ese panel.\nPackages: se muestran todos los paquetes que tenga instalados, además, si estos tienen un check significa que ya están cargados en la respectiva sesión. Cuando se instala R, se instalan varios paquetes por defecto.\nHelp: la mejor pestaña, cuando se está empezando en esto es un poco dificil de leer, pero es increíblemente útil y muchos de los errores que se pueden encontrar se pueden solucionar leyendo la documentación.\nOtros: por lo general, estas otras pestañas exceden esta guía, incluye Viewer y Presentation."
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#almacenamiento",
    "href": "tutoriales_R/introduccionR.html#almacenamiento",
    "title": "Introducción a R y RStudio",
    "section": "Almacenamiento",
    "text": "Almacenamiento\nCuando nombren a las variables, traten de usar un formato consistente y los mismos tienen que tener un significado “útil”. Eviten llamar a las cosas por letras o monosílabos.\n\n“Programar produce código y el código es una herramienta de comunicación. Obviamente, el código le dice a la computadora qué es lo que quiere que haga, pero también comunica significado a otros seres humanos. Es importante pensar el código como un medio de comunicación, ya que todo proyecto que realice es esencialmente colaborativo. Aun cuando no esté trabajando con otras personas, definitivamente lo estará haciendo con su futuro yo. Escribir código claro es importante para que otras personas (o usted en el futuro) puedan entender por qué se encaró un análisis de la manera que en la que se hizo. Esto significa que mejorando cómo programa, mejorará también cómo comunica. Con el tiempo querrá que su código resulte no solo más fácil de escribir, sino también más fácil de leer para los demás” (Wickham & Grolemund, 2017).\n\nCon “#” se hacen comentarios en el código, comentar es una excelente práctica, hágalo siempre. Describa lo que está haciendo y los motivos por los cuales lo está haciendo de ese modo.\nEl comando “Alt + -” produce esto “&lt;-” de forma rápida, que es una asignación. Lo que vamos a usar para almacenar variables, objetos, funciones, etc… por ejemplo:\n\nObserve los resultados en el panel de ambiente (environment)\n\n\nNúmeros\n\n# Números\n\nnumero &lt;- 10\n\nnuevo_numero &lt;- 5 + numero\n\n\n\nLetras\nEs necesario colocarlas entre comillas, pueden ser las simples o las dobles.\n\n# letras\n\nletra &lt;- \"a\"\n\nletra_2 &lt;- 'a' # no importa que tipo de comilla se usa, para R son iguales\n\n\nletra == letra_2\n\n[1] TRUE\n\n\n\n\nBooleanos\nEs decir, falso y verdadero, estos van en MAYÚSCULA.\n\n# note el cambio de color en el script respecto al resto del código\n\nFALSE\n\n[1] FALSE\n\nTRUE\n\n[1] TRUE\n\n# También se puede abreviar como F y T.\n\nF\n\n[1] FALSE\n\nT\n\n[1] TRUE\n\n\nCuando no hay data válida se utiliza NA (Not Available) o NaN (Not Available Number), esto puede tener varios significados, como:\n\nDatos faltantes porque se perdieron o no fueron recolectados, por ejemplo, la ausencia de datos en los años 2020-2021 en varias instituciones nacionales por la afectación de la pandemia COVID-19.\nDatos no recolectados por su ausencia de pertinencia: consumo eléctrico de una comunidad indígena sin acceso a servicios básicos de electricidad.\nOperaciones matemáticas indefinidas, por ejemplo \\(\\frac{0}{0} = NaN\\).\n\n\nNA\n\n[1] NA\n\nNaN\n\n[1] NaN\n\n0/0\n\n[1] NaN\n\n# Otros\n\nInf\n\n[1] Inf\n\n-Inf\n\n[1] -Inf\n\nNULL\n\nNULL\n\n\n\nNota: TRUE, FALSE, T, F, NA, NaN, for, if, else, entre otras, son palabras RESERVADAS en el lenguaje de programación R.\n\n\n\nVectores\nPara ello se usa la función c() que significa combine.\n\nvector_numerico &lt;- c(1, 2, 3)\nvector_numerico\n\n[1] 1 2 3\n\nis.numeric(vector_numerico)\n\n[1] TRUE\n\nvector_letras &lt;- c(\"A\", \"B\", \"C\")\n\nis.character(vector_letras)\n\n[1] TRUE\n\n# Si se juntan los dos vectores anteriores, R no da error, solo los convierte\n# a todos en caracteres, por lo que hay que tener cuidado. \n\nnuevo_vector &lt;- c(vector_numerico, vector_letras)\n\nnuevo_vector\n\n[1] \"1\" \"2\" \"3\" \"A\" \"B\" \"C\"\n\nis.numeric(nuevo_vector)\n\n[1] FALSE\n\nis.character(nuevo_vector)\n\n[1] TRUE\n\n# Se pueden crear vectores vacíos que luego se pueden llenar\n\nvector_vacio &lt;- c()\n\nvector_vacio2 &lt;- vector()\n\nvector_vacio[1] &lt;- 2 # [1] indica la posición 1 en el vector\n\nvector_vacio\n\n[1] 2\n\nvector_vacio[2] &lt;- 4 \n\nvector_vacio\n\n[1] 2 4\n\n\n\n\nOperaciones con vectores\n\nvector_numerico + 5\n\n[1] 6 7 8\n\nvector_numerico + c(5, 6, 7)\n\n[1]  6  8 10\n\nvector_numerico^2\n\n[1] 1 4 9\n\nvector_numerico/2\n\n[1] 0.5 1.0 1.5\n\nvector_numerico*9\n\n[1]  9 18 27\n\nvector_numerico/c(5, 6, 7)\n\n[1] 0.2000000 0.3333333 0.4285714\n\n\n\n\nSeleccionar elementos de un vector (slicing)\n\nvector_numerico[1]\n\n[1] 1\n\nvector_numerico[1:3]\n\n[1] 1 2 3\n\nvector_numerico[c(1,2,3)]\n\n[1] 1 2 3\n\nvector_letras[2]\n\n[1] \"B\"\n\nvector_letras[c(1, 3)]\n\n[1] \"A\" \"C\"\n\nvector_letras[-2]\n\n[1] \"A\" \"C\"\n\n\n\n\nSecuencias y repeticiones\n\n# Secuencias\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1, 10, by = 0.5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\nseq(1, 10, length = 5)\n\n[1]  1.00  3.25  5.50  7.75 10.00\n\n# Repeticiones\n\nrep(2, 4)\n\n[1] 2 2 2 2\n\nrep(vector_letras, 3)\n\n[1] \"A\" \"B\" \"C\" \"A\" \"B\" \"C\" \"A\" \"B\" \"C\"\n\nrep(vector_letras, each = 3)\n\n[1] \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"C\" \"C\" \"C\"\n\n\n\n\nMatrices\n\nmatriz1 &lt;- cbind(vector_numerico, vector_letras)\n\nmatriz1\n\n     vector_numerico vector_letras\n[1,] \"1\"             \"A\"          \n[2,] \"2\"             \"B\"          \n[3,] \"3\"             \"C\"          \n\nmatriz2 &lt;- cbind(rep(\"x\", 6), seq(1, 3, length = 6))\n\nmatriz2\n\n     [,1] [,2] \n[1,] \"x\"  \"1\"  \n[2,] \"x\"  \"1.4\"\n[3,] \"x\"  \"1.8\"\n[4,] \"x\"  \"2.2\"\n[5,] \"x\"  \"2.6\"\n[6,] \"x\"  \"3\"  \n\n\nEl slicing en matrices se hace con operaciones de tipo [row, column].\n\nmatriz1[1, ] # la primer fila y todas las columnas\n\nvector_numerico   vector_letras \n            \"1\"             \"A\" \n\nmatriz1[, 1] # la primer columna y todas las filas\n\n[1] \"1\" \"2\" \"3\"\n\nmatriz1[1, c(1, 2)] # la primer fila y las primeras dos columnas\n\nvector_numerico   vector_letras \n            \"1\"             \"A\" \n\nmatriz1[c(1, 3), ] # la fila 1 y 3 y todas las columnas\n\n     vector_numerico vector_letras\n[1,] \"1\"             \"A\"          \n[2,] \"3\"             \"C\"          \n\n\nEjemplos similares en la sección de data.frame().\n\n\nFunciones\nSe pueden crear, o usar las que ya existen en R y los respectivos paquetes.\n\nsum(vector_numerico) #suma\n\n[1] 6\n\nmax(vector_numerico) # máximo\n\n[1] 3\n\nmean(vector_numerico) # promedio\n\n[1] 2\n\nlength(vector_letras) # largo del vector\n\n[1] 3\n\n\n\n?mean()\nhelp(mean)\n\nPara ver la ayuda en el panel help, basta con ejecutar este comando: ?mean() o bien help(mean) en el script o en la consola.\nOtra forma de hacerlo es presionando la tecla F1 sobre la función.\n\n\nPaquetes\nHay un montón de paquetes con funciones adicionales, en este curso vamos a usar mucho tidyverse, se pueden instalar paquetes de diversas formas, por ejemplo:\n\nCorriendo la siguiente función install.packages(\"tidyverse\") en script o en consola.\nEl panel de Packages, con el botón “Install”, luego colocar el nombre del paquete e instalar.\n\n\n\n\n\n\n\n\n\n\n\n\nEn “Tools&gt;Install Packages…”"
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#proyectos",
    "href": "tutoriales_R/introduccionR.html#proyectos",
    "title": "Introducción a R y RStudio",
    "section": "Proyectos",
    "text": "Proyectos\nCree una carpeta con el nombre del proyecto, por ejemplo: “Clases”, dentro de dicha carpeta ordene su trabajo de una forma similar a esta:\n\nDatos\nImágenes\nResultados, etc.\n\nEsto queda a su criterio, pero sea ordenado. Este es un ejemplo de orden para una carpeta:\n\nLo importante es que mantenga un orden, con nombres claros y subcarpetas de ser necesario.\n\nDirectorio de trabajo\nR posee el poderoso concepto de directorio de trabajo (working directory en inglés). Aquí es donde R busca los archivos que le pide que lea y donde colocará todos los archivos que le pida que guarde.\n\n# Se utiliza para cambiar el directorio. Se debe colocar entre paréntesis\n# y entre comillas la dirección del directorio deseado. Por ejemplo:\n\nsetwd(\"C:/Users/Administrator/Desktop\")\n\n# para visualizar en qué directorio se está trabajando\n\ngetwd()\n\nPero este es un enfoque algo novato y anticuado. Lo práctico es crear un proyecto.\n\n\nCrear un proyecto\nLas personas expertas en R mantienen todos los archivos asociados a un proyecto en un mismo lugar — datos de entrada, scripts, resultados, gráficos. Esta es una práctica tan acertada y común, que RStudio cuenta con soporte integrado para esto por medio de los proyectos.\nUna vez creó la carpeta, se recomienda crear un proyecto, para que R siempre sepa donde está trabajando. En este caso el proyecto se llama igual que la carpeta: “Clases”. Arriba del panel de ambiente se pueden observar el proyecto.\n\nPor lo general dice “Project: (None)”, al dar clic sobre este se despliega lo siguiente, seleccionar “Existing Directory” y buscar la carpeta que crearon anteriormente y listo. Es aconsejable hacer esto para que cada proyecto que vayan a crear.\n\nCierre RStudio. Inspeccione la carpeta asociada a su proyecto — encontrará que hay un archivo .Rproj allí. Haz doble clic en ese archivo para reabrir el proyecto. Note que al hacer esto, vuelve al punto donde estaba justo antes de cerrar RStudio: es el mismo directorio de trabajo e historial de comandos, y todos los archivos con los que estabas trabajando siguen abiertos."
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#tidy-data-datos-ordenados",
    "href": "tutoriales_R/introduccionR.html#tidy-data-datos-ordenados",
    "title": "Introducción a R y RStudio",
    "section": "Tidy data (Datos ordenados)",
    "text": "Tidy data (Datos ordenados)\nEl análisis de datos nace de manejar datos, valga la redundancia. La forma en la que se expresan los datos pueden ser tan variados como los datos mismos y tener un estándar para trabajar los datos es casi tan importante como tener los mismos datos.\nTidy data es un estándar de manejo de datos, en donde el significados de los datos está expresado en su estructura. Se trabaja con datos rectangulares, es decir, tablas, donde:\n\nCada variable es una columna\nCada observación de la población estudiada es una fila\nCada tipo de información es una tabla.\n\nMás detalle lo pueden encontrar en este linK: https://r4ds.had.co.nz/tidy-data.html\nTidy data es sumamente importante en R, porque así es como se construyen las tablas en este lenguaje de programación."
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#tablas",
    "href": "tutoriales_R/introduccionR.html#tablas",
    "title": "Introducción a R y RStudio",
    "section": "Tablas",
    "text": "Tablas\nSe pueden escribir tanto a mano como usando vectores (como los creados previamente)\n\nvector_letras &lt;- c(\"D\", \"E\", \"F\")\nvector_numerico &lt;- c(4, 5, 6)\n\ntabla &lt;- data.frame(Columna1 = c(\"Fila 1\", \"Fila 2\", \"Fila 3\"),\n                    Columna2 = vector_letras,\n                    Columna3 = vector_numerico,\n                    \"Columna 4\" = rep(1,3))\n\ntabla\n\n  Columna1 Columna2 Columna3 Columna.4\n1   Fila 1        D        4         1\n2   Fila 2        E        5         1\n3   Fila 3        F        6         1\n\n\nPara acceder a las diferentes posiciones de una tabla, se puede hacer de diferentes formas:\n\n# por su nombre\n\ntabla$Columna1\n\n[1] \"Fila 1\" \"Fila 2\" \"Fila 3\"\n\ntabla$Columna2\n\n[1] \"D\" \"E\" \"F\"\n\ntabla[, \"Columna1\"]\n\n[1] \"Fila 1\" \"Fila 2\" \"Fila 3\"\n\n# por su posición\n\n## Primera fila y todas las columnas\n\ntabla[1, ] # [Fila, Columna]\n\n  Columna1 Columna2 Columna3 Columna.4\n1   Fila 1        D        4         1\n\n## Primera y segunda fila y todas las columnas\n\ntabla[c(1,2), ]\n\n  Columna1 Columna2 Columna3 Columna.4\n1   Fila 1        D        4         1\n2   Fila 2        E        5         1\n\n## Primera fila y segunda columna\n\ntabla[1, 2]\n\n[1] \"D\"\n\n## Primera columna\n\ntabla[, 1]\n\n[1] \"Fila 1\" \"Fila 2\" \"Fila 3\"\n\n\nY así, sucesivamente…\nTambién se pueden agregar columnas nuevas de esta forma:\n\ntabla$Columna4 &lt;- c(10, 20, 30)\n\ntabla$Estudiantes &lt;- c(\"X\", \"Y\", \"Z\")\n\ntabla\n\n  Columna1 Columna2 Columna3 Columna.4 Columna4 Estudiantes\n1   Fila 1        D        4         1       10           X\n2   Fila 2        E        5         1       20           Y\n3   Fila 3        F        6         1       30           Z\n\n\nLas tablas también se pueden cargar, de texto plano (.csv, .txt, .tsv, etc), MS Excel, objetos de R, entre otros.\nDescargar datos csv\n\nlibrary(readr)\n\n# esta forma de programar se puede denominar como explícita, en el tanto\n# especifica paquete::función()\n\nconsumo &lt;- readr::read_csv(\"datos/ConsumoElectrico.csv\")\n\nconsumo\n\n# A tibble: 74 × 4\n   Consumo Zona   Tipo Habitantes\n     &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1     274 A         1          3\n 2     186 A         1          7\n 3      49 A         1          4\n 4     249 A         1          4\n 5      76 A         1          3\n 6     315 A         1          4\n 7     194 A         1          6\n 8     113 A         0          2\n 9     121 A         0          4\n10     263 A         0          3\n# ℹ 64 more rows\n\n\n\nPara leer archivos de MS Excel vamos a usar este paquete y función: openxlsx::read.xlsx()\n\nDescargar datos xlsx\n\n# estos son los datos del primer reporte\n\njulio2022 &lt;- openxlsx::read.xlsx(\"datos/Julio2022.xlsx\")"
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#observaciones-finales",
    "href": "tutoriales_R/introduccionR.html#observaciones-finales",
    "title": "Introducción a R y RStudio",
    "section": "Observaciones finales",
    "text": "Observaciones finales\nVamos a dejar como estudio individual aspectos “clásicos” de la programación como for, if, else, while, etc. Bajo el supuesto de que son aspectos comunes a la mayoría de lenguajes de programación."
  },
  {
    "objectID": "tutoriales_R/introduccionR.html#carga-de-datos",
    "href": "tutoriales_R/introduccionR.html#carga-de-datos",
    "title": "Introducción a R y RStudio",
    "section": "Carga de datos",
    "text": "Carga de datos\nBien, ahora carguemos los datos:\nDescargar datos Notas\n\ndatos &lt;- readr::read_csv(\"datos/Notas.csv\")\n\ndatos\n\n# A tibble: 4 × 3\n  Nombre  Nota Sexo \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 Hugo    84.6 M    \n2 Paco    71.2 M    \n3 Luis    90.5 M    \n4 Daisy   96.7 F    \n\nhead(datos, 2) # los primeros 2 datos\n\n# A tibble: 2 × 3\n  Nombre  Nota Sexo \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 Hugo    84.6 M    \n2 Paco    71.2 M    \n\nnames(datos) # los nombres de las variables de los datos\n\n[1] \"Nombre\" \"Nota\"   \"Sexo\"  \n\ntail(datos, 2) # los últimos 2 datos\n\n# A tibble: 2 × 3\n  Nombre  Nota Sexo \n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 Luis    90.5 M    \n2 Daisy   96.7 F    \n\nstr(datos) # la estructura de datos\n\nspc_tbl_ [4 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Nombre: chr [1:4] \"Hugo\" \"Paco\" \"Luis\" \"Daisy\"\n $ Nota  : num [1:4] 84.6 71.2 90.5 96.7\n $ Sexo  : chr [1:4] \"M\" \"M\" \"M\" \"F\"\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Nombre = col_character(),\n  ..   Nota = col_double(),\n  ..   Sexo = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(datos) # resumen de los datos\n\n    Nombre               Nota           Sexo          \n Length:4           Min.   :71.20   Length:4          \n Class :character   1st Qu.:81.25   Class :character  \n Mode  :character   Median :87.55   Mode  :character  \n                    Mean   :85.75                     \n                    3rd Qu.:92.05                     \n                    Max.   :96.70"
  },
  {
    "objectID": "tutoriales_R/graficos_control.html",
    "href": "tutoriales_R/graficos_control.html",
    "title": "Gráficos de control de Shewhart para variables",
    "section": "",
    "text": "El control estadístico del proceso implica la comparación de la salida del proceso con un estándar y la toma de decisiones intermedias en caso de discrepancia entre las dos.\nTambién implica determinar si el proceso puede producir productos que cumplan con las especificaciones requeridas.\nLos resultados de todos los procesos, ya sean procesos de fabricación o procesos que proporcionan algún tipo de servicio, están sujetos a la variabilidad. La variabilidad hace que sea más difícil para los procesos generar productos que se ajusten a los límites de especificación deseados.\nShewhart (1931) definió las dos causas de variabilidad en los resultados del proceso como causas comunes y causas especiales o asignables.\n\nLas causas comunes de variabilidad se deben a la naturaleza inherente del proceso. No pueden eliminarse o reducirse sin cambiar el proceso real.\nLas causas atribuibles de la variabilidad, por otra parte, son interrupciones inusuales del funcionamiento normal. Deben identificarse y eliminarse para reducir la variabilidad y hacer que el proceso sea más capaz de cumplir las especificaciones.\n\n\n\nLos gráficos de control son herramientas estadísticas. Su uso es la forma más eficaz de distinguir entre causa común y atribuible para la variabilidad cuando se supervisa el resultado del proceso en tiempo real. Aunque los gráficos de control por sí solos no pueden reducir la variabilidad del proceso, pueden ayudar a prevenir una reacción excesiva a las causas comunes de la variabilidad (lo que puede empeorar las cosas) y ayudar a evitar ignorar las señales de causa asignables. Cuando se reconoce la presencia de una causa asignable para la variabilidad, el conocimiento del proceso puede llevar a ajustes para eliminar esta causa y reducir la variabilidad en los resultados del proceso.\n\n\nEn la fase I, se utilizan gráficos de control sobre datos retrospectivos para calcular los límites preliminares de control y determinar si el proceso había estado bajo control durante el período en que se recopilaron los datos. Cuando se detectan causas asignables en la carta de control utilizando los datos históricos, se lleva a cabo una investigación para encontrar la causa. Si se encuentra la causa y puede prevenirse en el futuro, se eliminan los datos correspondientes a los puntos fuera de control del gráfico y se vuelven a calcular los límites de control.\nEste es normalmente un proceso iterativo y se repite hasta que los límites de los gráficos de control se refinan, se produce ub gráfico que no parece contener ninguna causa asignable, y el proceso parece estar funcionando a un nivel aceptable. La información obtenida a partir del gráfico de control de la fase I se utiliza entonces como base para el seguimiento de la fase II. Dado que los límites del gráfico de control se calculan repetidamente en la fase I, los cálculos se realizan normalmente utilizando un ordenador."
  },
  {
    "objectID": "tutoriales_R/graficos_control.html#gráficos-de-control",
    "href": "tutoriales_R/graficos_control.html#gráficos-de-control",
    "title": "Gráficos de control de Shewhart para variables",
    "section": "",
    "text": "Los gráficos de control son herramientas estadísticas. Su uso es la forma más eficaz de distinguir entre causa común y atribuible para la variabilidad cuando se supervisa el resultado del proceso en tiempo real. Aunque los gráficos de control por sí solos no pueden reducir la variabilidad del proceso, pueden ayudar a prevenir una reacción excesiva a las causas comunes de la variabilidad (lo que puede empeorar las cosas) y ayudar a evitar ignorar las señales de causa asignables. Cuando se reconoce la presencia de una causa asignable para la variabilidad, el conocimiento del proceso puede llevar a ajustes para eliminar esta causa y reducir la variabilidad en los resultados del proceso.\n\n\nEn la fase I, se utilizan gráficos de control sobre datos retrospectivos para calcular los límites preliminares de control y determinar si el proceso había estado bajo control durante el período en que se recopilaron los datos. Cuando se detectan causas asignables en la carta de control utilizando los datos históricos, se lleva a cabo una investigación para encontrar la causa. Si se encuentra la causa y puede prevenirse en el futuro, se eliminan los datos correspondientes a los puntos fuera de control del gráfico y se vuelven a calcular los límites de control.\nEste es normalmente un proceso iterativo y se repite hasta que los límites de los gráficos de control se refinan, se produce ub gráfico que no parece contener ninguna causa asignable, y el proceso parece estar funcionando a un nivel aceptable. La información obtenida a partir del gráfico de control de la fase I se utiliza entonces como base para el seguimiento de la fase II. Dado que los límites del gráfico de control se calculan repetidamente en la fase I, los cálculos se realizan normalmente utilizando un ordenador."
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_normales.html",
    "href": "tutoriales_R/capacidad_datos_normales.html",
    "title": "Análisis de capacidad en R para datos normales",
    "section": "",
    "text": "Al finalizar la Fase I, cuando los gráficos de control hayan demostrado que el proceso se encuentra bajo control y en un nivel aceptable, se pueden calcular los PCR para documentar el estado del proceso."
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_normales.html#usando-qcc",
    "href": "tutoriales_R/capacidad_datos_normales.html#usando-qcc",
    "title": "Análisis de capacidad en R para datos normales",
    "section": "Usando qcc",
    "text": "Usando qcc\nLuego, se verifica que que el proceso se encuentre bajo control estadístico, para ello vamos a comenzar con un gráfico \\(\\bar{x}-R\\).\n\nX_R &lt;- qcc::qcc(datos, type = \"xbar\")\n\n\n\n\n\n\n\nplot(X_R)\n\nEl segundo requisito, es que los datos sigan la distribución normal. Por ejemplo, con un \\(\\alpha=0.001\\).\n\nas.matrix(datos) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.95336, p-value = 0.00139\n\n\nUna vez resuelto esto, se puede evaluar la capacidad del proceso.\n\nqcc::process.capability(X_R, spec.limits = c(7.115, 7.135), target = 7.125)\n\n\n\n\n\n\n\n\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = X_R, spec.limits = c(7.115,     7.135), target = 7.125)\n\nNumber of obs = 100          Target = 7.125\n       Center = 7.125           LSL = 7.115\n       StdDev = 0.002098        USL = 7.135\n\nCapability indices:\n\n      Value   2.5%  97.5%\nCp    1.589  1.368  1.809\nCp_l  1.573  1.381  1.765\nCp_u  1.605  1.409  1.800\nCp_k  1.573  1.344  1.801\nCpm   1.587  1.367  1.807\n\nExp&lt;LSL 0%   Obs&lt;LSL 0%\nExp&gt;USL 0%   Obs&gt;USL 0%"
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_normales.html#usando-sixsigma",
    "href": "tutoriales_R/capacidad_datos_normales.html#usando-sixsigma",
    "title": "Análisis de capacidad en R para datos normales",
    "section": "Usando SixSigma",
    "text": "Usando SixSigma\nEste paquete puede ser un poco más sencillo de usar. Pero necesita los datos en una sola columna.\n\ndatos_una_col &lt;- datos %&gt;% \n  tidyr::pivot_longer(everything())\n\n\nSixSigma::ss.study.ca(xST = datos_una_col$value, # corto plazo\n                      xLT = datos_una_col$value, # largo plazo\n                      Target = 7.115, LSL=7.135,USL=7.125)"
  },
  {
    "objectID": "tutoriales_R.html",
    "href": "tutoriales_R.html",
    "title": "Tutoriales en R",
    "section": "",
    "text": "En esta sección encontrará tutoriales desarrollados en R para la resolución de algunos ejercicios relacionados con probabilidad, estadística e ingeniería."
  },
  {
    "objectID": "tutoriales_R.html#aspectos-generales",
    "href": "tutoriales_R.html#aspectos-generales",
    "title": "Tutoriales en R",
    "section": "Aspectos generales ",
    "text": "Aspectos generales \n\nGuía básica de R: Introducción\n\nPara los que nunca han programado, o al menos nunca han programado en R/Rstudio."
  },
  {
    "objectID": "tutoriales_R.html#estadística",
    "href": "tutoriales_R.html#estadística",
    "title": "Tutoriales en R",
    "section": "Estadística ",
    "text": "Estadística \n\nDemostración del Teorema del Límite Central (TLC)"
  },
  {
    "objectID": "tutoriales_R.html#ingeniería-de-calidad",
    "href": "tutoriales_R.html#ingeniería-de-calidad",
    "title": "Tutoriales en R",
    "section": "Ingeniería de Calidad ",
    "text": "Ingeniería de Calidad \n\nDiagrama de Pareto\n\nAprenda a realizar un gráfico de Pareto en R. Este es un principio que dice que una pequeña cantidad de causas suele explicar la mayor parte de los efectos\n\nGráficos de control para variables\n\nLos gráficos de control son herramientas estadísticas. Su uso es la forma más eficaz de distinguir entre causa común y atribuible para la variabilidad cuando se supervisa el resultado del proceso en tiempo real.\n\nGráficos de control para atributos\n\nMientras que los gráficos de control para variables rastrean las cantidades medidas relacionadas con la calidad de los resultados del proceso, los gráficos para atributos siguen el recuento de los elementos no conformes.\n\nAnálisis de capacidad para datos normales\n\nUn análisis de capacidad es una herramienta estadística que evalúa si un proceso puede cumplir de forma consistente con los requisitos o especificaciones establecidos. Compara la variabilidad natural del proceso con los límites de tolerancia del cliente para determinar qué tan capaz es.\n\nAnálisis de capacidad para datos no normales\n\nEs relativamente común encontrarse con procesos que no siguen la distribución normal, ya sea por que así es como debe comportarse o bien, por errores que suceden a la hora de recolectar, almacenar o procesar los datos."
  },
  {
    "objectID": "recursos_graficos.html",
    "href": "recursos_graficos.html",
    "title": "Recursos para la construcción de gráficos",
    "section": "",
    "text": "En esta sección encontrará herramientas útiles que le permitirán seleccionar gráficos adecuados y evitar errores en la construcción de los mismos."
  },
  {
    "objectID": "recursos_graficos.html#guías-gráficas",
    "href": "recursos_graficos.html#guías-gráficas",
    "title": "Recursos para la construcción de gráficos",
    "section": "Guías gráficas",
    "text": "Guías gráficas\n\nSeleccione el gráfico adecuado\n\n¿Qué tipo de datos tiene? Elija el tipo principal usando esta guía.\n\nEvite estos errores\n\nUna colección de advertencias para la visualización de datos\n\nOtro catálogo de visualización de datos\n\nEncuentre otros gráficos y guías para su construcción."
  },
  {
    "objectID": "interactivos/distribuciones-discretas.html",
    "href": "interactivos/distribuciones-discretas.html",
    "title": "Distribuciones discretas interactivas",
    "section": "",
    "text": "Este visualizador permite explorar distribuciones discretas y ver cómo cambian sus formas al modificar los parámetros. Seleccione primero una distribución en el menú. Luego ajuste los controles (deslizadores) para cambiar sus valores.\nEl gráfico se actualiza automáticamente y muestra la función de probabilidad correspondiente. Observe cómo los parámetros afectan la posición del centro (moda), la dispersión y la asimetría de cada distribución. También permite la descarga en formato PNG, y al posar el cursor sobre las barras se despliega el valor de probabilidad asociado a ese valor.\n\n\n  \n    \n    \n    \n    \n  \n  \n  \n  \n  \n    \n  \n    \n      Seleccione una distribución:\n      \n        Binomial\n        Binomial negativa\n        Poisson\n        Hipergeométrica\n      \n    \n  \n    \n    \n  \n    \n      \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "interactivos/distribuciones-discretas.html#instrucciones-de-uso",
    "href": "interactivos/distribuciones-discretas.html#instrucciones-de-uso",
    "title": "Distribuciones discretas interactivas",
    "section": "",
    "text": "Este visualizador permite explorar distribuciones discretas y ver cómo cambian sus formas al modificar los parámetros. Seleccione primero una distribución en el menú. Luego ajuste los controles (deslizadores) para cambiar sus valores.\nEl gráfico se actualiza automáticamente y muestra la función de probabilidad correspondiente. Observe cómo los parámetros afectan la posición del centro (moda), la dispersión y la asimetría de cada distribución. También permite la descarga en formato PNG, y al posar el cursor sobre las barras se despliega el valor de probabilidad asociado a ese valor.\n\n\n  \n    \n    \n    \n    \n  \n  \n  \n  \n  \n    \n  \n    \n      Seleccione una distribución:\n      \n        Binomial\n        Binomial negativa\n        Poisson\n        Hipergeométrica\n      \n    \n  \n    \n    \n  \n    \n      \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "interactivos/demostracion_n-1.html",
    "href": "interactivos/demostracion_n-1.html",
    "title": "¿Por qué n-1? en la varianza muestral",
    "section": "",
    "text": "Demostración matemática de por qué se usa \\(n-1\\) en el denominador al calcular la varianza muestral. E() se conoce como esperanza y puede que al momento de consultar este sitio web aun no lo conozca.\n\\[\nE(s^2)=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\right)\n\\]\nSe reescribe \\(x_i-\\bar{x}=x_i-\\bar{x}-\\mu+\\mu\\), entonces:\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)-(\\bar{x}-\\mu)\\right)^2\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-2(x_i-\\mu)(\\bar{x}-\\mu)+(\\bar{x}-\\mu)^2\\right)\\right)\n\\]\nNote que \\(\\sum_{i=1}^{n}1=n\\), por tanto:\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2}{n}(\\bar{x}-\\mu)\\sum_{i=1}^{n}(x_i-\\mu)+\\frac{1}{n}(\\bar{x}-\\mu)^2\\sum_{i=1}^{n}1\\right)\\right)\n\\]\nTenemos que\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)=\\frac{1}{n}\\sum_{i=1}^{n}x_i-\\frac{n}{n}\\mu\n\\]\ny sabemos que el promedio\n\\[\n\\bar{x}=\\frac{\\sum_{i=1}^{n}x_i}{n}\n\\]\nEntonces:\n\\[\n\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)=\\bar{x}-\\mu\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2}{n}(\\bar{x}-\\mu)\\sum_{i=1}^{n}(\\bar{x}-\\mu)+\\left(\\frac{n}{n}(\\bar{x}-\\mu)\\right)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2}{n}n(\\bar{x}-\\mu)^2+(\\bar{x}-\\mu)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-2(\\bar{x}-\\mu)^2+(\\bar{x}-\\mu)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2\\right)+E\\left(-2(\\bar{x}-\\mu)^2+(\\bar{x}-\\mu)^2\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2\\right)-E\\left((\\bar{x}-\\mu)^2\\right)\n\\]\n\\[\n=\\frac{1}{n}\\sum_{i=1}^{n}E\\left((x_i-\\mu)^2\\right)-E\\left((\\bar{x}-\\mu)^2\\right)\n\\]\nPor otro lado, por la definición de la media \\(\\mu=E(x)\\) y por la definición de la varianza \\(\\sigma^2=E((x-E(x))^2)=\\mathrm{var}(x)\\). Entonces:\n\\[\n=\\frac{1}{n}\\sum_{i=1}^{n}\\mathrm{var}(x_i)-\\mathrm{var}(\\bar{x})\n\\]\nPor definición \\(\\mathrm{var}(\\bar{x})=\\frac{\\sigma^2}{n}\\) y \\(\\sum_{i=1}^{n}\\mathrm{var}(x_i)=\\sigma^2\\sum_{i=1}^{n}1=n\\sigma^2\\)\n\\[\n=\\frac{1}{n}\\cdot n\\sigma^2-\\frac{\\sigma^2}{n}\n\\]\n\\[\n=\\sigma^2-\\frac{\\sigma^2}{n}\n=\\sigma^2\\left(\\frac{n-1}{n}\\right)\\neq\\sigma^2\n\\]\nLo que produce un estimador SESGADO de la varianza.\nSi se usa \\(n-1\\) en lugar de \\(n\\), se produce el estimador insesgado\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}\\left((x_i-\\mu)-(\\bar{x}-\\mu)\\right)^2\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-2(x_i-\\mu)(\\bar{x}-\\mu)+(\\bar{x}-\\mu)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2}{n-1}(\\bar{x}-\\mu)\\sum_{i=1}^{n}(x_i-\\mu)+\\frac{1}{n-1}(\\bar{x}-\\mu)^2\\sum_{i=1}^{n}1\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2}{n-1}(\\bar{x}-\\mu)\\sum_{i=1}^{n}(\\bar{x}-\\mu)+\\left(\\frac{n}{n-1}(\\bar{x}-\\mu)\\right)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}\\left((x_i-\\mu)^2-\\frac{2n}{n-1}(\\bar{x}-\\mu)^2+\\frac{n}{n-1}(\\bar{x}-\\mu)^2\\right)\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\mu)^2\\right)+\\frac{n}{n-1}E\\left(-2(\\bar{x}-\\mu)^2+(\\bar{x}-\\mu)^2\\right)\n\\]\n\\[\n=E\\left(\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\mu)^2\\right)-\\frac{n}{n-1}E\\left((\\bar{x}-\\mu)^2\\right)\n\\]\n\\[\n=\\frac{1}{n-1}\\sum_{i=1}^{n}E\\left((x_i-\\mu)^2\\right)-\\frac{n}{n-1}E\\left((\\bar{x}-\\mu)^2\\right)\n\\]\n\\[\n=\\frac{1}{n-1}\\sum_{i=1}^{n}\\mathrm{var}(x_i)-\\frac{n}{n-1}\\mathrm{var}(\\bar{x})\n\\]\n\\[\n=\\frac{1}{n-1}\\cdot n\\sigma^2-\\frac{n}{n-1}\\frac{\\sigma^2}{n}\n\\]\n\\[\n=\\frac{n}{n-1}\\sigma^2-\\frac{1}{n-1}\\sigma^2\n\\]\n\\[\n=\\frac{n}{n-1}\\left(\\sigma^2-\\frac{\\sigma^2}{n}\\right)\n\\]\n\\[\n=\\frac{n}{n-1}\\left[\\sigma^2\\left(1-\\frac{1}{n}\\right)\\right]\n\\]\n\\[\n=\\frac{n}{n-1}\\left[\\sigma^2\\frac{n-1}{n}\\right]\n=\\sigma^2\n\\]"
  },
  {
    "objectID": "interactivos/convolucion_distribuciones.html",
    "href": "interactivos/convolucion_distribuciones.html",
    "title": "Convolución de distribuciones",
    "section": "",
    "text": "En estadística y probabilidad, convolucionar dos distribuciones es la forma correcta de responder a esta pregunta:\n\nSi X y Y son variables aleatorias independientes, ¿cuál es la distribución de su suma Z=X+Y?\n\nLa respuesta no se obtiene sumando puntos, ni promediando curvas, sino mediante una convolución (el cual es un tema que por lo general se aborda en Ecuaciones Diferenciales). Intuitivamente, la convolución toma una distribución, la “desliza” sobre la otra y mide cuánto se superponen. Cada posible forma de obtener un mismo valor de Z contribuye a su probabilidad total.\n\n\nAunque la definición es general, cada familia de distribuciones tiene comportamientos característicos.\n\nUniforme + Uniforme La suma de dos variables uniformes no es uniforme. Produce una distribución triangular (o trapezoidal si los intervalos no coinciden). La incertidumbre empieza a concentrarse hacia el centro.\nTriangular + Triangular La convolución suaviza aún más la forma. Aparecen distribuciones con perfiles poligonales de mayor orden, cada vez más parecidas a una campana de Gauss (Normal).\n\nLa aplicación permite ver estas transiciones de forma visual. Observe cómo curvas simples, al sumarse, se vuelven más suaves, más simétricas y más concentradas alrededor de un valor central.\n\nRelación con el Teorema de Límite Central\n\nEl TLC dice que, bajo ciertas circunstancias, que se dan normalmente en el mundo de la experimentación, la distribución muestral de la media tenderá a una distribución normal a medida que el número de componentes se haga más grande, con independencia de las distribuciones particulares de los componentes. Cuando se suman muchas variables aleatorias independientes (aunque no sean normales), estamos aplicando convoluciones una y otra vez.\n\n\n\nUna medición real suele ser una suma de diferentes contribuciones:\n\nResolución del instrumento\nError del método\nVariabilidad ambiental\nOperador\nCalibración\n\nCada una de estas fuentes puede modelarse como una variable aleatoria con su propia distribución, lo que justifica el uso extendido de la distribución normal en los informes de incertidumbre."
  },
  {
    "objectID": "interactivos/convolucion_distribuciones.html#introducción",
    "href": "interactivos/convolucion_distribuciones.html#introducción",
    "title": "Convolución de distribuciones",
    "section": "",
    "text": "En estadística y probabilidad, convolucionar dos distribuciones es la forma correcta de responder a esta pregunta:\n\nSi X y Y son variables aleatorias independientes, ¿cuál es la distribución de su suma Z=X+Y?\n\nLa respuesta no se obtiene sumando puntos, ni promediando curvas, sino mediante una convolución (el cual es un tema que por lo general se aborda en Ecuaciones Diferenciales). Intuitivamente, la convolución toma una distribución, la “desliza” sobre la otra y mide cuánto se superponen. Cada posible forma de obtener un mismo valor de Z contribuye a su probabilidad total.\n\n\nAunque la definición es general, cada familia de distribuciones tiene comportamientos característicos.\n\nUniforme + Uniforme La suma de dos variables uniformes no es uniforme. Produce una distribución triangular (o trapezoidal si los intervalos no coinciden). La incertidumbre empieza a concentrarse hacia el centro.\nTriangular + Triangular La convolución suaviza aún más la forma. Aparecen distribuciones con perfiles poligonales de mayor orden, cada vez más parecidas a una campana de Gauss (Normal).\n\nLa aplicación permite ver estas transiciones de forma visual. Observe cómo curvas simples, al sumarse, se vuelven más suaves, más simétricas y más concentradas alrededor de un valor central.\n\nRelación con el Teorema de Límite Central\n\nEl TLC dice que, bajo ciertas circunstancias, que se dan normalmente en el mundo de la experimentación, la distribución muestral de la media tenderá a una distribución normal a medida que el número de componentes se haga más grande, con independencia de las distribuciones particulares de los componentes. Cuando se suman muchas variables aleatorias independientes (aunque no sean normales), estamos aplicando convoluciones una y otra vez.\n\n\n\nUna medición real suele ser una suma de diferentes contribuciones:\n\nResolución del instrumento\nError del método\nVariabilidad ambiental\nOperador\nCalibración\n\nCada una de estas fuentes puede modelarse como una variable aleatoria con su propia distribución, lo que justifica el uso extendido de la distribución normal en los informes de incertidumbre."
  },
  {
    "objectID": "interactivos/convolucion_distribuciones.html#instrucciones-de-uso",
    "href": "interactivos/convolucion_distribuciones.html#instrucciones-de-uso",
    "title": "Convolución de distribuciones",
    "section": "Instrucciones de uso",
    "text": "Instrucciones de uso\nEste visualizador permite explorar la convolución de distribuciones continuas, es decir, cómo se combinan dos variables aleatorias independientes X y Y para obtener la distribución de su suma Z=X+Y. Primero seleccione una distribución para la variable X y una para la variable Y. Para cada variable, ajuste los parámetros correspondientes mediante los sliders.\nEl gráfico se actualiza automáticamente y muestra tres curvas:\n\nLa densidad de la variable X,\nLa densidad de la variable Y,\nLa densidad resultante de la suma Z = X + Y, obtenida mediante convolución.\n\nObserve cómo cambian la forma, la dispersión y la concentración de la distribución resultante al modificar los parámetros. En particular, note cómo la suma de distribuciones no normales puede producir perfiles cada vez más suaves y simétricos, anticipando el comportamiento descrito por el Teorema del Límite Central.\nEl gráfico es interactivo: puede acercar o alejar regiones, desplazar el plano y descargar el gráfico en formato PNG. Al posar el cursor sobre las curvas se muestran los valores aproximados de la densidad.\n\n\n\n  \n\n  \n\n\n\n\n\n\n  \n\n    \n      X\n      \n        Uniforme (Rectangular)\n        Triangular\n        Normal\n      \n      \n    \n\n    \n      Y\n      \n        Uniforme (Rectangular)\n        Triangular\n        Normal\n      \n      \n    \n\n  \n\n  \n\n\n\n      \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Febrero 2026"
  },
  {
    "objectID": "infografias.html",
    "href": "infografias.html",
    "title": "Infografías",
    "section": "",
    "text": "En esta sección encontrará infografías organizadas en pestañas para facilitar su consulta. Todo el material, exceptuando aquel etiquetado como externo, es de mi autoría."
  },
  {
    "objectID": "infografias.html#inferencia-estadística",
    "href": "infografias.html#inferencia-estadística",
    "title": "Infografías",
    "section": "Inferencia estadística ",
    "text": "Inferencia estadística \n\nComprendiendo los valores P\n\nEsta infografía presenta la definición del valor P, criterios para su interpretación y una guía para la elección del nivel de significancia (\\(\\alpha\\))."
  },
  {
    "objectID": "clases.html",
    "href": "clases.html",
    "title": "Material de clases",
    "section": "",
    "text": "Aquí encontrará las clases de los diferentes cursos del área de conocimiento Estadística, de la Escuela de Ingeniería Industrial"
  },
  {
    "objectID": "clases.html#ii-1120-estadística-para-ingeniería-industrial-i",
    "href": "clases.html#ii-1120-estadística-para-ingeniería-industrial-i",
    "title": "Material de clases",
    "section": "II-1120 Estadística para Ingeniería Industrial I",
    "text": "II-1120 Estadística para Ingeniería Industrial I\n\nIntroducción a la estadística\nFundamentos de muestreo y manejo de datos\nDescripción de datos (Estadística descriptiva)"
  },
  {
    "objectID": "clases.html#ii-1123-estadística-para-ingeniería-industrial-ii",
    "href": "clases.html#ii-1123-estadística-para-ingeniería-industrial-ii",
    "title": "Material de clases",
    "section": "II-1123 Estadística para Ingeniería Industrial II",
    "text": "II-1123 Estadística para Ingeniería Industrial II\n\nFundamentos de muestreo y manejo de datos\nPruebas de bondad de ajuste para distribuciones\nEstadística no paramétrica para una variable"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steven García Goñi",
    "section": "",
    "text": "Este sitio tiene como objetivo compartir mis proyectos y algunos recursos de enseñanza para los cursos que imparto, así como otros artículos sobre ingeniería, estadística y R (y algunas veces en Phyton).\n\n\n\n\n\nMi nombre es Steven García Goñi, soy de Costa Rica y por lo general las personas suelen llamarme Goñi. Tengo una Licenciatura en Ingeniería Industrial (2020) y una Maestría Académica en Estadística (2025), ambas por mi alma mater, la Universidad de Costa Rica.\nMe gusta la lectura de libros clásicos, principalmente rusos, y filosofía. Mi autor favorito es Dostoievski. Recientemente estoy ampliando mis horizontes hacia la literatura japonesa, mis comienzos estuvieron marcados por Poe y el realismo mágico. Además, me encanta la panadería, por lo que dedico buena parte de mi tiempo libre en experimentar en la elaboración de panes.\n\nAll models are wrong, but some are useful - George Box\n\n\nIt’s easy to lie with statistics, but it’s hard to tell the truth without them &gt; - Andrejs Dunkels\n\n\nWithout data, you’re just another person with an opinion - W. Edwards Deming\n\n\n\n\nDurante mis estudios de Licenciatura, desde 2016, trabajé durante los interciclos universitarios en la creación, mantenimiento y actualización de un Sistema Integrado de Gestión (SIG) con las normas AS9100 e ISO 9001, logrando ser una de las primeras empresas en Costa Rica en conseguirlo. Desde el 2018 hasta el 2021 fue mi labor a tiempo completo, incluyendo la automatización de tareas repetitivas.\nGracias a esta experiencia, del 2020 al 2023 formé parte del Comité Técnico Nacional 061 - Aeroespacial para el desarrollo y revisión de los trabajos de normalización en el campo aeroespacial, incluyendo la adaptación de 21 normas internacionales.\nEntre otras actividades, hice servicios profesionales en auditorías de segunda parte de la norma AS9100D para PyMEs de Costa Rica y fui consultor y analista de datos.\n\n\n\nDesde el 2022 soy profesor e investigador en la Escuela de Ingeniería Industrial de la Universidad de Costa Rica, donde me desempeño como docente de cursos como\n\nProbabilidad y estadística\nEstadística Industrial 1 (Nueva estructura curricular)\nEstadística Industrial 2 (Nueva estructura curricular)\nDiseño de experimentos (DoE) básico\nIngeniería de Calidad\nDiseño de experimentos (DoE) avanzado\nEntre otros relacionados con estadística y machine learning\n\nEn 2025 tuve la oportunidad de impartir Tópicos de Diseño Experimental para el Programa de Posgrado de Estadística de la UCR.\n\n\n\nMis intereses de investigación se pueden resumir en dos grandes aristas:\n\nLa aplicación de estadística industrial\nEl diseño estadístico de experimentos (DoE)\n\nNo obstante, me considero una persona abierta a explorar nuevas áreas."
  },
  {
    "objectID": "index.html#brevemente",
    "href": "index.html#brevemente",
    "title": "Steven García Goñi",
    "section": "",
    "text": "Mi nombre es Steven García Goñi, soy de Costa Rica y por lo general las personas suelen llamarme Goñi. Tengo una Licenciatura en Ingeniería Industrial (2020) y una Maestría Académica en Estadística (2025), ambas por mi alma mater, la Universidad de Costa Rica.\nMe gusta la lectura de libros clásicos, principalmente rusos, y filosofía. Mi autor favorito es Dostoievski. Recientemente estoy ampliando mis horizontes hacia la literatura japonesa, mis comienzos estuvieron marcados por Poe y el realismo mágico. Además, me encanta la panadería, por lo que dedico buena parte de mi tiempo libre en experimentar en la elaboración de panes.\n\nAll models are wrong, but some are useful - George Box\n\n\nIt’s easy to lie with statistics, but it’s hard to tell the truth without them &gt; - Andrejs Dunkels\n\n\nWithout data, you’re just another person with an opinion - W. Edwards Deming"
  },
  {
    "objectID": "index.html#qué-he-hecho",
    "href": "index.html#qué-he-hecho",
    "title": "Steven García Goñi",
    "section": "",
    "text": "Durante mis estudios de Licenciatura, desde 2016, trabajé durante los interciclos universitarios en la creación, mantenimiento y actualización de un Sistema Integrado de Gestión (SIG) con las normas AS9100 e ISO 9001, logrando ser una de las primeras empresas en Costa Rica en conseguirlo. Desde el 2018 hasta el 2021 fue mi labor a tiempo completo, incluyendo la automatización de tareas repetitivas.\nGracias a esta experiencia, del 2020 al 2023 formé parte del Comité Técnico Nacional 061 - Aeroespacial para el desarrollo y revisión de los trabajos de normalización en el campo aeroespacial, incluyendo la adaptación de 21 normas internacionales.\nEntre otras actividades, hice servicios profesionales en auditorías de segunda parte de la norma AS9100D para PyMEs de Costa Rica y fui consultor y analista de datos."
  },
  {
    "objectID": "index.html#qué-hago",
    "href": "index.html#qué-hago",
    "title": "Steven García Goñi",
    "section": "",
    "text": "Desde el 2022 soy profesor e investigador en la Escuela de Ingeniería Industrial de la Universidad de Costa Rica, donde me desempeño como docente de cursos como\n\nProbabilidad y estadística\nEstadística Industrial 1 (Nueva estructura curricular)\nEstadística Industrial 2 (Nueva estructura curricular)\nDiseño de experimentos (DoE) básico\nIngeniería de Calidad\nDiseño de experimentos (DoE) avanzado\nEntre otros relacionados con estadística y machine learning\n\nEn 2025 tuve la oportunidad de impartir Tópicos de Diseño Experimental para el Programa de Posgrado de Estadística de la UCR."
  },
  {
    "objectID": "index.html#mis-intereses-en-investigación",
    "href": "index.html#mis-intereses-en-investigación",
    "title": "Steven García Goñi",
    "section": "",
    "text": "Mis intereses de investigación se pueden resumir en dos grandes aristas:\n\nLa aplicación de estadística industrial\nEl diseño estadístico de experimentos (DoE)\n\nNo obstante, me considero una persona abierta a explorar nuevas áreas."
  },
  {
    "objectID": "interactivos.html",
    "href": "interactivos.html",
    "title": "Simulaciones y visualizadores interactivos",
    "section": "",
    "text": "En esta sección encontrará simulaciones interactivas y herramientas útiles para cursos de probabilidad, estadística e ingeniería. Todo el material, exceptuando aquel etiquetado como externo, es de mi autoría."
  },
  {
    "objectID": "interactivos.html#recursos-interactivos",
    "href": "interactivos.html#recursos-interactivos",
    "title": "Simulaciones y visualizadores interactivos",
    "section": "Recursos interactivos ",
    "text": "Recursos interactivos \n\nDistribuciones discretas interactivas\n\nEste visualizador le permite explorar las principales distribuciones discretas y ver como cambian sus formas al modificar los parámetros.\n\nDistribuciones continuas interactivas\n\nEs el equivalente continuo al visualizador anterior, pero además, presenta una comparación entre la distribución normal estándar (\\(z\\)) y la t-Student (\\(t\\)).\n\nConvolución de distribuciones\n\nEste visualizador interactivo permite explorar cómo se combinan dos variables aleatorias independientes mediante la convolución de sus distribuciones. Muestra la distribución resultante de la suma y su relación con el Teorema del Límite Central.\n\nCurvas OC para muestreo por atributos\n\nPropio del campo de la ingeniería de calidad y muestreo de aceptación, este visualizador muestra las curvas características de operación (OC) de planes de muestreo por atributos.\n\nEfecto de la asimetría sobre el TLC\n\nInspirado en algunas publicaciones científicas, aquí podrá encontrar como la asimetría juega un papel ponderante en el cumplimiento del teorema del límite central (TLC). Con este, busco ayudar a “derrumbar” el mito de que el TLC siempre se cumple con \\(n=30\\).\n\nCalculadora de distribuciones de probabilidad e intervalos de confianza - Aplicación externa\n\nDe Ph.D. Matt Bognar, aquí encuentra “mini aplicaciones” con las que puede resolver ejercicios de probabilidad y estadística.\n\nLey de los Grandes Números - Aplicación externa\n\nEs una explicación de Rafael Pérez Laserna sobre la Ley de los Grandes Números, que incluye una simulación de un dado que nos ayuda a comprenderla.\n\nTeorema del Límite Central (TLC) - Aplicación externa\n\nUno de los pilares de la estadística frecuentista, este enlace muestra una simulación del TLC que nos permite comprender el TLC de mejor forma.\n\nAplicación interactiva sobre los intervalos de confianza\n\nEste es un gist que al ejecutarse en R con sus respectivas librerías, despliega una aplicación local que permite comprender mejor el funcionamiento de los intervalos de confianza y como estos dependen del estimador puntual (aleatorio por el muestreo), el tamaño de muestra y el nivel de confianza seleccionado.\n\n# Correr en R (requiere shiny y tidyverse)\n\nlibrary(shiny)\nlibrary(tidyverse)\n\nrunGist(\"d47f4c205b531e3bed59d22b22cbfd13\")"
  },
  {
    "objectID": "interactivos/curvas-oc.html",
    "href": "interactivos/curvas-oc.html",
    "title": "Curvas OC - Muestreo por atributos",
    "section": "",
    "text": "Este visualizador permite explorar las curvas características de planes de muestreo por atributos. Al seleccionar una distribución (Binomial, Poisson o Hipergeométrica), puede ajustar los parámetros del plan y observar cómo cambian tres curvas asociadas:\n\nLa curva OC (Operating Characteristic) muestra la probabilidad de aceptar un lote en función de la fracción defectuosa. Ilustra el equilibrio entre el riesgo del productor (α) y el riesgo del consumidor (β).\nLa curva AOQ (Average Outgoing Quality) muestra la fracción defectuosa esperada después de inspección, útil para evaluar la calidad promedio de salida.\nLa curva ITP (Inspección Total Promedio) presenta la relación entre la calidad del material entrante y el número de elementos que se deben inspeccionar, suponiendo que los lotes rechazados se inspeccionarán en un 100% y se realizará una inspección de rectificación de los elementos defectuosos.\n\nAjuste los parámetros del plan: tamaño de muestra (n), nivel de aceptación (c), tamaño del lote (N) y los riesgos α y β. Cada modificación actualiza automáticamente las curvas. También se marcan los puntos AQL (Acceptable Quality Level) y LQL (Lot Tolerance Percent Defective) cuando corresponden, lo que facilita interpretar las regiones típicas de desempeño del plan.\nObserve cómo cada distribución induce una forma distinta de la curva OC:la Binomial modela defectos independientes en muestras moderadas; la Poisson simplifica el caso de eventos raros; y la Hipergeométrica considera inspección sin reemplazo, capturando la finitud del lote.\n\n\n\n  \n  \n  \n  \n\n\n\n  \n\n  Seleccione una distribución:\n  \n    Binomial\n    Hipergeométrica\n    Poisson\n    Comparar (solo OC)\n  \n\n  \n\n  \n    \n      \n        \n        \n      \n  \n\n  \n\n      \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "interactivos/curvas-oc.html#instrucciones-de-uso",
    "href": "interactivos/curvas-oc.html#instrucciones-de-uso",
    "title": "Curvas OC - Muestreo por atributos",
    "section": "",
    "text": "Este visualizador permite explorar las curvas características de planes de muestreo por atributos. Al seleccionar una distribución (Binomial, Poisson o Hipergeométrica), puede ajustar los parámetros del plan y observar cómo cambian tres curvas asociadas:\n\nLa curva OC (Operating Characteristic) muestra la probabilidad de aceptar un lote en función de la fracción defectuosa. Ilustra el equilibrio entre el riesgo del productor (α) y el riesgo del consumidor (β).\nLa curva AOQ (Average Outgoing Quality) muestra la fracción defectuosa esperada después de inspección, útil para evaluar la calidad promedio de salida.\nLa curva ITP (Inspección Total Promedio) presenta la relación entre la calidad del material entrante y el número de elementos que se deben inspeccionar, suponiendo que los lotes rechazados se inspeccionarán en un 100% y se realizará una inspección de rectificación de los elementos defectuosos.\n\nAjuste los parámetros del plan: tamaño de muestra (n), nivel de aceptación (c), tamaño del lote (N) y los riesgos α y β. Cada modificación actualiza automáticamente las curvas. También se marcan los puntos AQL (Acceptable Quality Level) y LQL (Lot Tolerance Percent Defective) cuando corresponden, lo que facilita interpretar las regiones típicas de desempeño del plan.\nObserve cómo cada distribución induce una forma distinta de la curva OC:la Binomial modela defectos independientes en muestras moderadas; la Poisson simplifica el caso de eventos raros; y la Hipergeométrica considera inspección sin reemplazo, capturando la finitud del lote.\n\n\n\n  \n  \n  \n  \n\n\n\n  \n\n  Seleccione una distribución:\n  \n    Binomial\n    Hipergeométrica\n    Poisson\n    Comparar (solo OC)\n  \n\n  \n\n  \n    \n      \n        \n        \n      \n  \n\n  \n\n      \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "interactivos/distribuciones-continuas.html",
    "href": "interactivos/distribuciones-continuas.html",
    "title": "Distribuciones continuas interactivas",
    "section": "",
    "text": "Este visualizador permite explorar distribuciones continuas y observar cómo cambian sus formas al modificar sus parámetros. Seleccione primero una distribución en el menú desplegable. Luego ajuste los controles (deslizadores) para cambiar los valores de los parámetros correspondientes.\nEl gráfico se actualiza automáticamente y muestra la función de densidad de la distribución seleccionada. Observe cómo los parámetros afectan la posición central (media o moda), la dispersión y la asimetría de cada distribución. También puede descargar el gráfico en formato PNG y al posar el cursor sobre la curva se despliega el valor de densidad asociado a cada punto.\nPor otro lado, también le permite comparar la forma de la distribución Normal estándar con la distribución t-Student para distintos valores de los grados de libertad. Ambas distribuciones aparecen en inferencia estadística, pero juegan papeles distintos: la Normal representa un escenario con información completa y varianza conocida, mientras que la t-Student surge cuando la varianza se estima a partir de muestras pequeñas.\nLa diferencia más visible está en la cola de la distribución. Para grados de libertad pequeños, la t-Student presenta colas más gruesas, lo que refleja mayor incertidumbre y una mayor probabilidad relativa de valores extremos. Conforme los grados de libertad aumentan, la t-Student se aproxima rápidamente a la Normal y ambas curvas se vuelven prácticamente indistinguibles. Use el deslizador para ajustar los grados de libertad y observe cómo cambia la forma de la t-Student respecto a la Normal estándar.\n\n\n\n  \n  \n  \n  \n\n\n\n  \n\n  \n    Seleccione una distribución:\n    \n      Normal\n      Exponencial\n      Gamma\n      Weibull\n      Lognormal\n      Beta\n      Comparación: Normal vs t-Student\n    \n\n  \n  \n\n  \n\n  \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "interactivos/distribuciones-continuas.html#instrucciones-de-uso",
    "href": "interactivos/distribuciones-continuas.html#instrucciones-de-uso",
    "title": "Distribuciones continuas interactivas",
    "section": "",
    "text": "Este visualizador permite explorar distribuciones continuas y observar cómo cambian sus formas al modificar sus parámetros. Seleccione primero una distribución en el menú desplegable. Luego ajuste los controles (deslizadores) para cambiar los valores de los parámetros correspondientes.\nEl gráfico se actualiza automáticamente y muestra la función de densidad de la distribución seleccionada. Observe cómo los parámetros afectan la posición central (media o moda), la dispersión y la asimetría de cada distribución. También puede descargar el gráfico en formato PNG y al posar el cursor sobre la curva se despliega el valor de densidad asociado a cada punto.\nPor otro lado, también le permite comparar la forma de la distribución Normal estándar con la distribución t-Student para distintos valores de los grados de libertad. Ambas distribuciones aparecen en inferencia estadística, pero juegan papeles distintos: la Normal representa un escenario con información completa y varianza conocida, mientras que la t-Student surge cuando la varianza se estima a partir de muestras pequeñas.\nLa diferencia más visible está en la cola de la distribución. Para grados de libertad pequeños, la t-Student presenta colas más gruesas, lo que refleja mayor incertidumbre y una mayor probabilidad relativa de valores extremos. Conforme los grados de libertad aumentan, la t-Student se aproxima rápidamente a la Normal y ambas curvas se vuelven prácticamente indistinguibles. Use el deslizador para ajustar los grados de libertad y observe cómo cambia la forma de la t-Student respecto a la Normal estándar.\n\n\n\n  \n  \n  \n  \n\n\n\n  \n\n  \n    Seleccione una distribución:\n    \n      Normal\n      Exponencial\n      Gamma\n      Weibull\n      Lognormal\n      Beta\n      Comparación: Normal vs t-Student\n    \n\n  \n  \n\n  \n\n  \n      Creado por Steven García Goñi · Para la Escuela de Ingeniería Industrial de la Universidad de Costa Rica · Actualizado: Enero 2026"
  },
  {
    "objectID": "logo.html",
    "href": "logo.html",
    "title": "Personal brand",
    "section": "",
    "text": "El logo de esta página fue diseñado por Mónica Umaña, fundadora de Musaline Studio.\nPero esto no es solo un logo, sino que es una representación que busca unir mis dos pasiones académicas: la ingeniería y la estadística. Está inspirado en los hex stickers que son una tradición dentro de la comunidad de , principalmente por influencia del tidyverse, como detalla Hadley Wickham en este artículo. Como supongo que se sospecha,  es mi herramienta predilecta para el desarrollo de mis actividades académicas.\n\n Escudo histórico del valle de Goñi (Navarra), que le da origen al apellido\n\nPor otro lado, Goñi es mi segundo apellido y tradicionalmente a todos en nuestra familia se nos conoce por el mismo. Aun cuando llegue a lugares nuevos, poco a poco adoptan la costumbre de llamarnos Goñi, por lo que era esencial que formara parte de mi imagen personal.\nCon esa identidad en mente, el logo incorpora un símbolo que representa cómo pienso y trabajo: al cuervo (de la familia córvidos). Es un símbolo de la curiosidad y creatividad; observan, prueban y corrigen; en este sentido funcionan como ingenieros e ingenieras, ya que recogen información, elaboran estrategias y buscan mejores resultados; por ejemplo:\n\nLos cuervos de Nueva Caledonia (Corvus moneduloides) fabrican y ajustan herramientas para extraer alimento. Experimentos clásicos de Gavin R. Hunt y colaboradores muestran modificación y transporte de herramientas, lo que implica planificación. https://doi.org/10.1038/379249a0\nTrabajos de Alex Taylor y colaboradores muestran que los cuervos infieren relaciones causales simples, distinguen pruebas exitosas de fallidas y ajustan el comportamiento en consecuencia. https://doi.org/10.1073/pnas.1208724109\nEstudios sobre córvidos en ambientes urbanos muestran estrategias de minimización de esfuerzo, por ejemplo, investigaciones en Japón sobre cuervos que utilizan el tráfico para romper nueces. https://doi.org/10.3838/jjo.68.43\n\nDe esta manera, este logo representa una forma de creatividad que no se limita a imaginar, sino que experimenta y aprende. La ingeniería industrial y la estadística buscan exactamente eso: entender los datos, modelar fenómenos, iterar soluciones y optimizar recursos. El cuervo encarna ese mismo método: medir el mundo, reducir la incertidumbre y mejorar procesos.\nAdemás, me encanta Edgar Allan Poe."
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_no_normales.html",
    "href": "tutoriales_R/capacidad_datos_no_normales.html",
    "title": "Análisis de capacidad para datos no normales",
    "section": "",
    "text": "El paquete qualityTools ya no está disponible para instalación directa desde RStudio, sino que debe instalarse de la siguiente forma. Lo mismo sucede para el paquete Johnson.\n\nlibrary(devtools)\n\ndevtools::install_github(\"cran/qualityTools\")\ndevtools::install_github(\"hrbrmstr/Johnson\")"
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_no_normales.html#transformaciones-no-lineales",
    "href": "tutoriales_R/capacidad_datos_no_normales.html#transformaciones-no-lineales",
    "title": "Análisis de capacidad para datos no normales",
    "section": "Transformaciones no lineales",
    "text": "Transformaciones no lineales\nEl primer abordaje corresponde al uso de transformaciones no lineales.\n\nBox-Cox\nRecuerde que esta familia de transformaciones sigue la siguiente fórmula.\n\\[\nw_i=\\Biggl\\{ \\begin{matrix} x_i^\\lambda \\quad si \\quad \\lambda \\ne 0 \\\\ log(x_i) \\quad si \\quad \\lambda = 0\\end{matrix}, \\text{para } x_i &gt; 0\n\\]\n\nbc &lt;- MASS::boxcox(lm(datos_no_normales ~ 1))\n\n\n\n\n\n\n\nlambda &lt;- bc$x[which.max(bc$y)]\n\nlambda\n\n[1] 0.4242424\n\n\nAlgunos software trabajan directamente con los valores redondeados, por lo que en este caso \\(\\lambda=-2\\)\nAhora, apliquemos la transformación\n\n# Sin redondeo\ndatos_transformados_1 &lt;- datos_no_normales^lambda\n\nshapiro.test(datos_transformados_1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos_transformados_1\nW = 0.99547, p-value = 0.9855\n\n# Con redondeo\ndatos_transformados_2 &lt;- datos_no_normales^0.5\nshapiro.test(datos_transformados_2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos_transformados_2\nW = 0.99494, p-value = 0.9735\n\n\nNótese como en ambas situaciones, los datos ahora si siguen la distribución normal. Ahora analicemos la capacidad.\n\nXR &lt;- matrix(datos_transformados_1, ncol = 4) %&gt;% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Box - Cox\")\n\n\n\n\n\n\n\nqcc::process.capability(XR, \n                        spec.limits = c(1, 5),\n                        target = 3, \n                        breaks = \"sturges\")\n\n\n\n\n\n\n\n\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(1, 5), target = 3,     breaks = \"sturges\")\n\nNumber of obs = 100          Target = 3\n       Center = 1.458           LSL = 1\n       StdDev = 0.44            USL = 5\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    1.5152  1.3043  1.7257\nCp_l  0.3467  0.2786  0.4149\nCp_u  2.6837  2.3652  3.0021\nCp_k  0.3467  0.2655  0.4280\nCpm   0.4157  0.3359  0.4953\n\nExp&lt;LSL 15%  Obs&lt;LSL 14%\nExp&gt;USL 0%   Obs&gt;USL 0%\n\n\nPero hay algo raro, ¿no? Es porque aún no transformamos los límites y el nominal.\n\n# ponga atención, a veces la transformación invierte los límites\n\nc(1, 3, 5)^lambda\n\n[1] 1.000000 1.593731 1.979396\n\nqcc::process.capability(XR, \n                        spec.limits = c(1, 1.979396),\n                        target = 1.593731, \n                        breaks = \"sturges\")\n\n\n\n\n\n\n\n\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(1, 1.979396),     target = 1.593731, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 1.594\n       Center = 1.458           LSL = 1\n       StdDev = 0.44            USL = 1.979\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3710  0.3194  0.4225\nCp_l  0.3467  0.2786  0.4149\nCp_u  0.3952  0.3235  0.4669\nCp_k  0.3467  0.2655  0.4280\nCpm   0.3544  0.3033  0.4055\n\nExp&lt;LSL 15%  Obs&lt;LSL 14%\nExp&gt;USL 12%  Obs&gt;USL 13%\n\n\n\n\nJohnson\n\nNota: este paquete Johnson fue removido del CRAN (repositorio oficial), y en general suele ser algo deficiente en el cálculo de las transformaciones.\n\nEsta está compuesta por tres familias de transformaciones: \\(S_B, S_L, S_U\\).\n\ndatos_transformados_3 &lt;- Johnson::RE.Johnson(datos_no_normales)\n\nshapiro.test(datos_transformados_3$transformed)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos_transformados_3$transformed\nW = 0.99375, p-value = 0.929\n\n\nHabiendo verificado que la transformación es efectiva, podemos proceder a realizar el análisis de capacidad.\n\n# verificamos la familia de transformaciones\n\ndatos_transformados_3$`function`\n\n[1] \"SB\"\n\n\nSabiendo que es \\(S_B\\), se puede proceder según la fórmula\n\\[\nw_i = \\gamma+\\eta\\cdot ln\\Big[\\frac{x_i-\\varepsilon}{\\lambda+\\varepsilon-x_i}\\Big]\n\\]\nEntonces:\n\nprint(g &lt;- datos_transformados_3$f.gamma)\n\n[1] 3.081273\n\nprint(l &lt;- datos_transformados_3$f.lambda)\n\n[1] 24.67992\n\nprint(eta &lt;- datos_transformados_3$f.eta)\n\n[1] 1.690125\n\nprint(ep &lt;- datos_transformados_3$f.epsilon)\n\n[1] -0.9426102\n\nx_i &lt;- c(1, 3, 5)\n\ng + (eta * log((x_i-ep)/(l+ep-x_i)))  #log = ln\n\n[1] -1.0763915  0.2755111  1.1403874\n\n\nPor lo tanto:\n\nXR &lt;- matrix(datos_transformados_3$transformed, ncol = 4) %&gt;% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Johnson\")\n\n\n\n\n\n\n\nqcc::process.capability(XR, \n                        spec.limits = c(-1.0763915, 1.1403874),\n                        target = 0.2755111, \n                        breaks = \"sturges\")\n\n\n\n\n\n\n\n\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(-1.0763915,     1.1403874), target = 0.2755111, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 0.2755\n       Center = -0.02526        LSL = -1.076\n       StdDev = 0.9774          USL = 1.14\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3780  0.3254  0.4305\nCp_l  0.3585  0.2895  0.4275\nCp_u  0.3975  0.3256  0.4694\nCp_k  0.3585  0.2762  0.4407\nCpm   0.3613  0.3091  0.4133\n\nExp&lt;LSL 14%  Obs&lt;LSL 14%\nExp&gt;USL 12%  Obs&gt;USL 13%\n\n\n\n\nTransformación en dos pasos\nEn Excel también puede encontrar una forma de abordar este caso, pues es relativamente sencillo de aplicar.\nDescargar Transformación en dos pasos con Excel\nDada algunas pequeñas diferencias en los algoritmos programados, encontrará disimilitudes en algunos de los resultados obtenidos.\n\ndos_pasos &lt;- bestNormalize::bestNormalize(datos_no_normales)\n\ndatos_transformados_4 &lt;- dos_pasos$x.t * sd(datos_no_normales) +\n  mean(datos_no_normales)\n\nshapiro.test(datos_transformados_4)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos_transformados_4\nW = 0.98477, p-value = 0.3055\n\n\nComo puede observar, la transformación es efectiva. Ahora se deben obtener los límites.\n\nlimites &lt;- predict(dos_pasos, \n                   newdata = c(1, 3, 5), \n                   inverse = F) * sd(datos_no_normales) + \n  mean(datos_no_normales)\n\nlimites\n\n[1] 0.7706979 3.5210154 4.9708862\n\n\nResuelto esto, calculemos la capacidad\n\nXR &lt;- matrix(datos_transformados_4, ncol = 4) %&gt;% \n  qcc::qcc(type = \"xbar\", data.name = \"Datos transformados: Dos pasos\")\n\n\n\n\n\n\n\nqcc::process.capability(XR, \n                        spec.limits = c(0.7706979, 4.9708862),\n                        target = 3.5210154, \n                        breaks = \"sturges\")\n\n\n\n\n\n\n\n\n\nProcess Capability Analysis\n\nCall:\nqcc::process.capability(object = XR, spec.limits = c(0.7706979,     4.9708862), target = 3.5210154, breaks = \"sturges\")\n\nNumber of obs = 100          Target = 3.521\n       Center = 2.813           LSL = 0.7707\n       StdDev = 1.835           USL = 4.971\n\nCapability indices:\n\n       Value    2.5%   97.5%\nCp    0.3814  0.3283  0.4344\nCp_l  0.3709  0.3010  0.4408\nCp_u  0.3919  0.3204  0.4633\nCp_k  0.3709  0.2877  0.4542\nCpm   0.3559  0.3035  0.4081\n\nExp&lt;LSL 13%  Obs&lt;LSL 14%\nExp&gt;USL 12%  Obs&gt;USL 13%"
  },
  {
    "objectID": "tutoriales_R/capacidad_datos_no_normales.html#método-de-los-percentiles",
    "href": "tutoriales_R/capacidad_datos_no_normales.html#método-de-los-percentiles",
    "title": "Análisis de capacidad para datos no normales",
    "section": "Método de los percentiles",
    "text": "Método de los percentiles\nPara aplicar este método primero debe identificarse la distribución subyacente que mejor se ajusta a los datos. Esto se puede hacer de muchas formas, esta es una de ellas.\n\nfitdistrplus::descdist(datos_no_normales)\n\n\n\n\n\n\n\n\nsummary statistics\n------\nmin:  0.05970522   max:  10.59864 \nmedian:  2.457949 \nmean:  2.813116 \nestimated sd:  1.911338 \nestimated skewness:  1.168548 \nestimated kurtosis:  4.996266 \n\n\nDel gráfico se muestra que la Weibull, lognormal y gamma son posibles candidatos.\n\n# probamos varias distribuciones\n\ndistribuciones &lt;- c(\"weibull\", \"lnorm\", \"exp\", # agregar más si es conveniente\n  \"gamma\", \"unif\", \"logis\") %&gt;% \n  purrr::map(function(x){\n    fitdistrplus::fitdist(datos_no_normales, x)\n    })\n\nbondad_ajuste &lt;- fitdistrplus::gofstat(distribuciones)\n\nround(bondad_ajuste$chisqpvalue,3) # valores p\n\n1-mle-weibull   2-mle-lnorm     3-mle-exp   4-mle-gamma    5-mle-unif \n        0.920         0.051         0.015         0.755         0.000 \n  6-mle-logis \n        0.798 \n\nbondad_ajuste$kstest # decisiones kolmogorov\n\n 1-mle-weibull    2-mle-lnorm      3-mle-exp    4-mle-gamma     5-mle-unif \n\"not rejected\" \"not rejected\"     \"rejected\" \"not rejected\"     \"rejected\" \n   6-mle-logis \n\"not rejected\" \n\nbondad_ajuste$adtest # decisiones anderson - darling\n\n 1-mle-weibull    2-mle-lnorm      3-mle-exp    4-mle-gamma     5-mle-unif \n\"not rejected\" \"not computed\"     \"rejected\" \"not rejected\" \"not computed\" \n   6-mle-logis \n    \"rejected\" \n\n\nObservando los resultados se intuye que se puede usar la Weibull o la gamma. Como ya sabemos que los datos provenienen de una Weibull (además de su alto valor P), se va a trabajar con ello.\n\nEstimación con qualityTools\n\nEste paquete no está en CRAN, su instalación es manual. Suele presentar errores en versiones actuales de R.\n\n\nresultado &lt;- qualityTools::cp(datos_no_normales,\n                 distribution = \"weibull\", \n                 lsl = 1,\n                 target = 3,\n                 usl = 5)\n\n\n    Anderson Darling Test for weibull distribution\n\ndata:  datos_no_normales \n\n\nError in round(x$statistic, 4): non-numeric argument to mathematical function\n\n\n\n\n\n\n\n\n\n\n\nEstimación manual\nA grosso modo, recordemos que la fórmula es:\n\\[\nC_p(q) = \\frac{USL - LSL}{x_{0.99865}-x_{0.00135}}\n\\]\nCon sus respectivas variantes para \\(C_{pk}\\).\nSabiendo que la distribución es Weibull, se van a estimar sus parámetros.\n\nfitdistrplus::fitdist(datos_no_normales, \"weibull\")\n\nFitting of the distribution ' weibull ' by maximum likelihood \nParameters:\n      estimate Std. Error\nshape 1.494667  0.1165100\nscale 3.110131  0.2188012\n\npercentiles &lt;- qweibull(c(0.99865, 0.00135, 0.5), # superior, inferior, mediana \n                        shape = 1.494667, scale = 3.110131)\n\n# Entonces Cp\n\n(5-1)/(percentiles[1]-percentiles[2])\n\n[1] 0.3648482\n\n# Y por tanto Cpk\n\nmin((5-percentiles[3])/(percentiles[1]-percentiles[3]), #cpu\n    (percentiles[3]-1)/(percentiles[3]-percentiles[2])) #cpl\n\n[1] 0.2995426"
  },
  {
    "objectID": "tutoriales_R/diagrama_pareto.html",
    "href": "tutoriales_R/diagrama_pareto.html",
    "title": "Gráfico de Pareto",
    "section": "",
    "text": "Este es un ejemplo sobre como llevar a cabo gráficos de Pareto en R, usando la librería qcc. Este ejercicio es extraído del material del profesor Ing. Manrique Araya Alfaro."
  },
  {
    "objectID": "tutoriales_R/diagrama_pareto.html#introducción",
    "href": "tutoriales_R/diagrama_pareto.html#introducción",
    "title": "Gráfico de Pareto",
    "section": "",
    "text": "Este es un ejemplo sobre como llevar a cabo gráficos de Pareto en R, usando la librería qcc. Este ejercicio es extraído del material del profesor Ing. Manrique Araya Alfaro."
  },
  {
    "objectID": "tutoriales_R/diagrama_pareto.html#librerías",
    "href": "tutoriales_R/diagrama_pareto.html#librerías",
    "title": "Gráfico de Pareto",
    "section": "Librerías",
    "text": "Librerías\n\nlibrary(tidyverse)\nlibrary(qcc)"
  },
  {
    "objectID": "tutoriales_R/diagrama_pareto.html#ejercicio",
    "href": "tutoriales_R/diagrama_pareto.html#ejercicio",
    "title": "Gráfico de Pareto",
    "section": "Ejercicio",
    "text": "Ejercicio\nEn una fábrica de aparatos de línea blanca se han presentado problemas con la calidad de las lavadoras. Un grupo de mejora de la calidad decide revisar los problemas de la tina de las lavadoras, ya que con frecuencia es necesario retrabajarlas para que éste tenga una calidad aceptable. Para ello, estratificaron los problemas en la tina de lavadora por tipo de defecto, con la idea de localizar cual es desperfecto principal. A continuación, se muestra el análisis de los defectos encontrados en las tinas producidas en 5 meses.\nRealice un análisis de Pareto y obtenga conclusiones.\n\nDefecto &lt;- c(\"Boca ovalada\", \"Perforaciones deformes\",\n             \"Boda despostillada\", \"Falta de fundente\",\n             \"Mal soldada\")\n\nFrecuencia &lt;- c(1200, 400, 180, 130, 40)\n\ndata.frame(Defecto, Frecuencia) %&gt;%\n  flextable::flextable(cwidth = 1.9) # este código es solo para presentar los datos\n\nDefectoFrecuenciaBoca ovalada1,200Perforaciones deformes400Boda despostillada180Falta de fundente130Mal soldada40\n\n\nAhora, realizamos el diagrama de Pareto.\n\n# Sin nombres\n\nqcc::pareto.chart(Frecuencia)\n\n\n\n\n\n\n\n\n   \nPareto chart analysis for Frecuencia\n      Frequency   Cum.Freq.  Percentage Cum.Percent.\n  A 1200.000000 1200.000000   61.538462    61.538462\n  B  400.000000 1600.000000   20.512821    82.051282\n  C  180.000000 1780.000000    9.230769    91.282051\n  D  130.000000 1910.000000    6.666667    97.948718\n  E   40.000000 1950.000000    2.051282   100.000000\n\n# Con nombres\n\nnames(Frecuencia) &lt;- Defecto\n\nqcc::pareto.chart(Frecuencia)\n\n\n\n\n\n\n\n\n                        \nPareto chart analysis for Frecuencia\n                           Frequency   Cum.Freq.  Percentage Cum.Percent.\n  Boca ovalada           1200.000000 1200.000000   61.538462    61.538462\n  Perforaciones deformes  400.000000 1600.000000   20.512821    82.051282\n  Boda despostillada      180.000000 1780.000000    9.230769    91.282051\n  Falta de fundente       130.000000 1910.000000    6.666667    97.948718\n  Mal soldada              40.000000 1950.000000    2.051282   100.000000"
  },
  {
    "objectID": "tutoriales_R/graficos_control_atributos.html",
    "href": "tutoriales_R/graficos_control_atributos.html",
    "title": "Gráficos de control para atributos",
    "section": "",
    "text": "Mientras que los gráficos de control para variables rastrean las cantidades medidas relacionadas con la calidad de los resultados del proceso, los gráficos para atributos siguen el recuento de los elementos no conformes. Los gráficos para atributos no son tan informativos como los gráficos para variables para los estudios de la fase I. Un desplazamiento por encima o por debajo del límite de control inferior o una serie de puntos por encima o por debajo de la línea central en un gráfico de variables puede dar una pista sobre la causa. Sin embargo, un cambio en el número de elementos no conformes puede dar tal indicio. Aún así los gráficos de atributos tienen valor en la fase I.\nEn las industrias de servicios y otras áreas no manufactureras, los datos de conteo pueden ser abundantes pero las mediciones numéricas raras. Además, se pueden considerar simultáneamente muchas características de los resultados del proceso utilizando gráficos de atributos."
  },
  {
    "objectID": "tutoriales_R/graficos_control_atributos.html#gráfico-p",
    "href": "tutoriales_R/graficos_control_atributos.html#gráfico-p",
    "title": "Gráficos de control para atributos",
    "section": "Gráfico \\(p\\)",
    "text": "Gráfico \\(p\\)\nA continuación se le presenta un ejemplo donde se han recogido \\(n=50\\) durante 30 periodos de tiempo.\n\ndata(orangejuice)\n\nhead(orangejuice)%&gt;% \n  flextable::flextable()\n\nsampleDsizetrial11250TRUE21550TRUE3850TRUE41050TRUE5450TRUE6750TRUE\n\ntail(orangejuice)%&gt;% \n  flextable::flextable()\n\nsampleDsizetrial49650FALSE50750FALSE51550FALSE52650FALSE53350FALSE54550FALSE\n\n\nSolo necesitamos, para el ejemplo, los primeros 30 datos (los de trial = TRUE).\n\ndatos &lt;- orangejuice %&gt;% \n  dplyr::filter(trial)\n\nCon este código podemos generar el gráfico.\n\np &lt;- qcc(datos$D, \n         sizes = 50, type = \"p\", \n         title = \"Gráfico de control p\")\n\n\n\n\n\n\n\n\nSi \\(n\\) es variable, supongamos que de esta manera (en un caso real, debe ingresar datos reales, estos son simulados):\n\nn_var &lt;- sample(40:60, # n, aleatorio, entre 40 y 60\n                30, # 30 datos\n                replace = TRUE) \n\nn_var\n\n [1] 51 45 56 57 46 55 51 48 57 44 50 52 58 54 51 55 51 50 43 49 50 40 55 58 53\n[26] 47 59 56 56 43\n\n\nEntonces\n\np &lt;- qcc(datos$D, \n         sizes = n_var, type = \"p\", \n         title = \"Gráfico de control p (n variable)\")"
  },
  {
    "objectID": "tutoriales_R/graficos_control_atributos.html#gráfico-np",
    "href": "tutoriales_R/graficos_control_atributos.html#gráfico-np",
    "title": "Gráficos de control para atributos",
    "section": "Gráfico \\(np\\)",
    "text": "Gráfico \\(np\\)\nNo hay muchas diferencias entre la generación del gráfico \\(p\\) y el \\(np\\), como podemos apreciar.\n\nnp &lt;- qcc(datos$D, \n          sizes = 50, type = \"np\", \n          title = \"Gráfico de control np\")\n\n\n\n\n\n\n\n\nEste también puede ser variable, pero no se recomienda y justo este gráfico a continuación es un prueba de ello.\n\nnp &lt;- qcc(datos$D, \n          sizes = n_var, type = \"np\", \n          title = \"Gráfico de control np (n variable)\")"
  },
  {
    "objectID": "tutoriales_R/graficos_control_atributos.html#gráfico-c",
    "href": "tutoriales_R/graficos_control_atributos.html#gráfico-c",
    "title": "Gráficos de control para atributos",
    "section": "Gráfico \\(c\\)",
    "text": "Gráfico \\(c\\)\nPara este ejemplo se va a emplear los siguientes datos\n\ndata(\"circuit\")\n\nhead(circuit)%&gt;% \n  flextable::flextable()\n\nxsizetrial21100TRUE24100TRUE16100TRUE12100TRUE15100TRUE5100TRUE\n\n\nAl igual que en los ejemplos de \\(p\\) y \\(np\\), solo necesitamos, para el ejemplo, los primeros 26 datos (los de trial = TRUE).\n\ndatos_circuito &lt;- circuit %&gt;% \n  dplyr::filter(trial)\n\nUna vez realizado este paso miscelaneo, se puede construir el gráfico de control \\(c\\)\n\nc &lt;- qcc(datos_circuito$x, type = \"c\", \n         title = \"Gráfico de control c\")\n\n\n\n\n\n\n\n\nPara este gráfico tampoco se recomienda \\(n\\) variable."
  },
  {
    "objectID": "tutoriales_R/graficos_control_atributos.html#gráfico-u",
    "href": "tutoriales_R/graficos_control_atributos.html#gráfico-u",
    "title": "Gráficos de control para atributos",
    "section": "Gráfico \\(u\\)",
    "text": "Gráfico \\(u\\)\nEmpleando los datos anteriores, se construye el gráfico de la siguiente manera.\n\nu &lt;- qcc(datos_circuito$x, \n         sizes = 100, type = \"u\", \n         title = \"Gráfico de control u\")\n\n\n\n\n\n\n\n\nEn caso de tener \\(n\\) variable (simulado a partir de los siguientes datos).\n\nn_var &lt;- sample(90:110, 26, replace = T)\n\nSe puede obtener el gráfico de esta manera\n\nu &lt;- qcc(datos_circuito$x, \n         sizes = n_var, # datos variables\n         type = \"u\", \n         title = \"Gráfico de control u (n variable)\")"
  },
  {
    "objectID": "tutoriales_R/Teorema del límite central.html",
    "href": "tutoriales_R/Teorema del límite central.html",
    "title": "Teorema del límite central",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(BSDA)\nlibrary(samplingbook)\nlibrary(EnvStats)\nlibrary(pwr)\nlibrary(car)"
  },
  {
    "objectID": "tutoriales_R/Teorema del límite central.html#distribución-muestral-de-la-media",
    "href": "tutoriales_R/Teorema del límite central.html#distribución-muestral-de-la-media",
    "title": "Teorema del límite central",
    "section": "Distribución muestral de la media",
    "text": "Distribución muestral de la media\nSupongamos que la muestra aleatoria proviene de una población normal.\nDicha distribución tiene unos parámetros \\(\\mu\\) y \\(\\sigma\\), que son poblacionales. Siendo \\(X_1, \\cdots, X_n\\) una muestra aleatoria que proviene de dicha población tendrá parámetros conocidos o desconocidos \\(\\bar{x}\\) y \\(s\\).\nPor ejemplo, tomemos una distribución normal con \\(\\mu = 5\\) y \\(\\sigma = 2\\)\n\\[\nx \\sim N(5, 2)\n\\]\nCambiemos gradualmente el tamaño de muestra y observemos el comportamiento del tamaño de la muestra sobre el parámetro muestral. Veamos como al aumentar N, la media de la muestra se vuelve más cercana a 5.\n\ndatos &lt;- 2:2000 %&gt;% \n  purrr::map_dfr(function(N){\n    \n    \n    muestra &lt;- rnorm(N, 5, 2)\n    \n    data.frame(N, Media = mean(muestra), Var = var(muestra))\n    \n    \n  }) \n\ndatos %&gt;% \n  ggplot2::ggplot(aes(x = N, y = Media)) +\n  geom_line() +\n  geom_hline(yintercept = 5, color = \"#331114\") +\n  labs(title = \"Efecto del tamaño de muestra\",\n       subtitle = \"Media\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSi la muestra proviene de poblaciones no normales o desconocidas, se puede aplicar el Teorema del Límite Central (TLC)."
  },
  {
    "objectID": "tutoriales_R/Teorema del límite central.html#distribución-muestral-de-la-varianza",
    "href": "tutoriales_R/Teorema del límite central.html#distribución-muestral-de-la-varianza",
    "title": "Teorema del límite central",
    "section": "Distribución muestral de la varianza",
    "text": "Distribución muestral de la varianza\nSe sigue de igual forma que con la sección anterior.\n¿Por qué el intercepto en este gráfico es 4 y no 2? Si la variable aleatoria es \\(x \\sim N(5, 2)\\).\n\ndatos %&gt;% \n  ggplot2::ggplot(aes(x = N, y = Var)) +\n  geom_line() +\n  geom_hline(yintercept = 4, color = \"#331114\") +\n  labs(title = \"Efecto del tamaño de muestra\",\n       subtitle = \"Varianza\") +\n  theme_bw()"
  },
  {
    "objectID": "tutoriales_R/Teorema del límite central.html#ejemplo-01",
    "href": "tutoriales_R/Teorema del límite central.html#ejemplo-01",
    "title": "Teorema del límite central",
    "section": "Ejemplo 01",
    "text": "Ejemplo 01\nSupongamos un experimento de lanzamiento de dados, cada cara está igual de cargada, por lo que la probabilidad de obtener cualquier cara es de 1/6.\n\n# así sería lanzar el dado 5 veces\n\nsample((1:6), # valores posibles (las caras del dado)\n       5, # cantidad de lanzamientos\n       replace = TRUE, # puede salir el mismo valor varias veces\n       prob = rep(1/6, 6)) # probabilidad para valor de las caras del dado\n\n[1] 1 3 1 1 6\n\n\nAhora, repitamos el lanzamiento, 5000 veces y observemos la distribución de los datos, la frecuencia relativa de cada lanzamiento debería ser aproximadamente 1/6 = 16.67 %.\n\nlanzamientos &lt;- sample(1:6, 5000, replace = TRUE, prob = rep(1/6, 6))\n\ntabla &lt;- summarytools::freq(lanzamientos)\n\ndata.frame(tabla) %&gt;% \n  tibble::rownames_to_column() %&gt;% \n  dplyr::select(1:4) %&gt;% \n  setNames(c(\"Valor del dado\", \"Frecuencia\", \"Frecuencia (%)\", \n             \"Frecuencia acumulada (%)\")) %&gt;% \n  knitr::kable(format = \"html\") %&gt;%\n  kableExtra::kable_styling()\n\n\n\n\nValor del dado\nFrecuencia\nFrecuencia (%)\nFrecuencia acumulada (%)\n\n\n\n\n1\n834\n16.68\n16.68\n\n\n2\n863\n17.26\n33.94\n\n\n3\n817\n16.34\n50.28\n\n\n4\n819\n16.38\n66.66\n\n\n5\n865\n17.30\n83.96\n\n\n6\n802\n16.04\n100.00\n\n\n&lt;NA&gt;\n0\nNA\nNA\n\n\nTotal\n5000\n100.00\n100.00\n\n\n\n\n\nEsto quiere decir que la distribución de los datos es uniforme discreta (ojo que no continua). Por lo que la cantidad de veces que se va a obtener una cara es:\n\\[\nFrecuencia = \\frac{1}{6}\\cdot 5000 = 833.33\n\\]\n\ndata.frame(lanzamientos) %&gt;% \n  ggplot2::ggplot(aes(lanzamientos)) + \n  geom_bar(fill = \"#331114\") +\n  geom_hline(yintercept = 833.33, size = 2, color = \"#9BA1AB\") +\n  scale_x_continuous(breaks = 1:6) +\n  labs(x = \"Lanzamientos\", \n       y = \"Frecuencia\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAhora expandamos el experimento, supongamos que vamos a extraer una muestra de \\(n=5\\) observaciones de la población anterior (el lanzamiento de un dado) y que se muestreo se repite en 1000 ocasiones. Para cada una de esas 5 observaciones se obtiene la media.\nSiguiendo el TLC, la forma resultante de este muestreo es la de una campana de Gauss (aproximadamente).\nPor ejemplo:\n\n# comencemos por hacer el experimento 10 veces\n\nfor(i in 1:10){\n  \n  dado &lt;- sample(1:6, 5, # lanzar 5 veces\n                 replace = TRUE, prob = rep(1/6, 6))\n  \n  print(mean(dado)) # calcular la media\n  \n}\n\n[1] 3.2\n[1] 3.4\n[1] 3.2\n[1] 3\n[1] 2.6\n[1] 2.4\n[1] 4.2\n[1] 3.6\n[1] 3.4\n[1] 2.6\n\n\nAhora si, repitámoslo 1000 veces\n\nexperimento_dado &lt;- data.frame() # data frame vacío\n\nfor(i in 1:1000){\n  \n  dado &lt;- sample(1:6, 5, # lanzar 5 veces\n                 replace = TRUE, prob = rep(1/6, 6))\n  \n  experimento_dado[i, 1] &lt;- mean(dado) # rellenamos el data frame\n  \n}\n\n# Y ahora observemos si toma forma de campana.\n\n\nhist(experimento_dado$V1,\n     main = \"Experimento 01: Media de 5 lanzamientos de dado, 1000 veces\",\n     xlab = \"Media de 5 lanzamientos\",\n     ylab = \"Frecuencia\",\n     col = \"#331114\", \n     border = \"#9BA1AB\")\n\n\n\n\n\n\n\n\n\n¿Cuál es la media de los datos del experimento?\n\nSe sabe, que la media (esperanza matemática de un dado) es 3.5. La media del experimento generado, debería ser aproximadamente igual.\n\nmean(experimento_dado$V1)\n\n[1] 3.531\n\n\n\n¿Cuál es la varianza de los datos del experimento?\n\nSe sabe, que para un dado perfecto, la varianza es:\n\\[\n\\frac{1}{6}\\cdot\\sum_{i=1}^6 (x_i - 3.5)^2 = 2.92\n\\]\nY por tanto la desviación es: 1.71. Siguiendo la fórmula anterior:\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{1.71}{\\sqrt{5}} = 0.765\n\\]\nLa desviación estándar del experimento debería ser aproximadamente similar:\n\nsd(experimento_dado$V1)\n\n[1] 0.7782317\n\n\n\nFinalmente\n\nAunque la distribución de la población es uniforme discreta, vemos como la de las medias muestrales se asemeja a una distribución normal.\nEn una situación real tendríamos sólo los estadísticos de nuestra muestra:\n\\[\n\\bar{X} \\sim N(3.531, 0.7782317)\n\\]\nGracias a TLC podemos acercarnos a conocer las características de la población, mediante la toma de muestras.\nPor ejemplo, la media poblacional y la media muestral son similares. La desviación poblacional \\(\\sigma\\) puede obtenerse al despejar esta fórmula \\(\\frac{\\sigma}{\\sqrt{n}} = s\\).\n\n¿Qué pasa si se aumenta \\(n\\)?\n\nLa aproximación se hace más precisa.\n\nFinalmente\n\n\\[\nZ = \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]"
  }
]